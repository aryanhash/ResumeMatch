# üèóÔ∏è AutoApply AI Backend Architecture - Complete Line-by-Line Explanation

## üìã Table of Contents

1. [System Overview](#system-overview)
2. [Cline CLI - Agent Code Generator](#cline-cli---agent-code-generator)
3. [Main Pipeline (`main.py`)](#main-pipeline-mainpy)
4. [Together AI Integration](#together-ai-integration)
5. [OUMI Integration](#oumi-integration)
6. [Kestra Orchestration](#kestra-orchestration)
7. [Data Flow Diagram](#data-flow-diagram)
8. [Agent-by-Agent Breakdown](#agent-by-agent-breakdown)

---

## üéØ System Overview

**Four-Tier Architecture:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CLINE (Code Generation Layer)                          ‚îÇ
‚îÇ  - Generates agent code from prompts                     ‚îÇ
‚îÇ  - Ensures consistency and bug fixes                     ‚îÇ
‚îÇ  - Maintains ethical guardrails                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  KESTRA (Orchestration Layer)                           ‚îÇ
‚îÇ  - Manages workflow execution                            ‚îÇ
‚îÇ  - Handles retries, timeouts, dependencies              ‚îÇ
‚îÇ  - Provides observability                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FASTAPI BACKEND (Application Layer)                    ‚îÇ
‚îÇ  - REST API endpoints                                    ‚îÇ
‚îÇ  - Agent orchestration                                   ‚îÇ
‚îÇ  - Business logic                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI AGENTS (Intelligence Layer)                         ‚îÇ
‚îÇ  - Together AI: LLM for parsing, rewriting, analysis     ‚îÇ
‚îÇ  - OUMI: Fine-tuned ATS classifier                       ‚îÇ
‚îÇ  - Rule-based: Skill matching, scoring                  ‚îÇ
‚îÇ  - Generated by: Cline CLI                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ü§ñ Cline CLI - Agent Code Generator

### **What is Cline?**

**Cline** is an AI-powered code generation tool that automatically generates backend agent code and FastAPI endpoints based on detailed prompt templates. Instead of manually writing every agent from scratch, Cline uses Claude (Anthropic's LLM) to generate production-ready code with:

**‚ö†Ô∏è IMPORTANT: Cline is a DEVELOPMENT-TIME tool only. It is NOT used at runtime.**

- **Development Time:** Cline generates code (one-time cost: ~$5-20 for all agents)
- **Runtime:** Generated agents use Together AI (not Cline)
- **No Ongoing Cline Costs:** Once code is generated and committed, Cline is not needed

- ‚úÖ Structured agent classes
- ‚úÖ Pydantic input/output schemas
- ‚úÖ Error handling
- ‚úÖ Ethical guardrails
- ‚úÖ Type hints and docstrings
- ‚úÖ CLI support

**Why Use Cline?**
- **Speed:** Generates complete agents in seconds
- **Consistency:** All agents follow the same structure
- **Quality:** Includes bug fixes and best practices
- **Maintainability:** Easy to regenerate with updated requirements

---

### **Cline Configuration (`cline/cline.config.json`)**

```json
{
  "name": "autoapply-ai",
  "version": "1.0.0",
  "description": "Cline configuration for AutoApply AI agent generation",
  
  "generator": {
    "model": "claude-3-opus",      // LLM model for code generation
    "temperature": 0.2,             // Low temperature for consistency
    "maxTokens": 4000               // Max tokens per generation
  },
  
  "templates": {
    "agent": {
      "outputDir": "../backend/agents",  // Where to save agent files
      "filePattern": "{name}.py",        // File naming pattern
      "includes": [
        "pydantic models",              // Always include Pydantic schemas
        "type hints",                    // Type annotations
        "docstrings",                    // Google-style docstrings
        "error handling",                // Try/except blocks
        "CLI support"                    // Command-line interface
      ]
    },
    "endpoint": {
      "outputDir": "../backend",
      "filePattern": "routes/{name}.py",
      "includes": [
        "FastAPI router",               // FastAPI route handlers
        "OpenAPI docs",                 // Swagger documentation
        "validation",                   // Input validation
        "error responses"               // HTTP error handling
      ]
    }
  },
  
  "prompts": {
    "resume_parser": "prompts/resume_parser.md",
    "jd_analyzer": "prompts/jd_analyzer.md",
    "ats_scorer": "prompts/ats_scorer.md",
    "cover_letter": "prompts/cover_letter.md",
    "fastapi_endpoint": "prompts/fastapi_endpoint.md"
  },
  
  "conventions": {
    "codeStyle": "black",              // Code formatter
    "docstringFormat": "google",        // Google-style docstrings
    "importOrder": ["stdlib", "thirdparty", "local"],
    "typeHints": true,                 // Require type hints
    "pydanticV2": true                 // Use Pydantic v2
  },
  
  "integrations": {
    "together_ai": {
      "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "envKey": "TOGETHER_API_KEY"
    },
    "kestra": {
      "namespace": "autoapplyai",
      "envKey": "KESTRA_API_URL"
    }
  }
}
```

**Configuration Breakdown:**

1. **Generator Settings:**
   - `model`: Uses Claude 3 Opus for code generation
   - `temperature`: 0.2 (low for consistent output)
   - `maxTokens`: 4000 (enough for complete agent)

2. **Templates:**
   - `agent`: Generates agent classes in `backend/agents/`
   - `endpoint`: Generates FastAPI routes in `backend/`

3. **Prompts:**
   - Maps prompt names to markdown files
   - Each prompt defines requirements and fixes

4. **Conventions:**
   - Enforces code style (Black formatter)
   - Requires type hints and docstrings
   - Uses Pydantic v2 for validation

5. **Integrations:**
   - Tells Cline about Together AI and Kestra
   - Ensures generated code uses correct APIs

---

### **How Cline Works: Prompt ‚Üí Code**

**Step 1: Create Prompt Template**

Each agent has a detailed prompt in `cline/prompts/`:

```markdown
# Cline Prompt: Resume Parser Agent

## Objective
Generate a Python agent that parses resumes into structured, validated JSON.

## Critical Issues to Fix

### Issue #1: No Field Validation
```python
# ‚ùå WRONG - accepts whatever LLM returns
return ParsedResume(**llm_response)

# ‚úÖ CORRECT - validate required fields
if not parsed.email:
    raise ValueError("Resume must have email address")
```

### Issue #2: Skills Not Deduplicated
```python
# ‚ùå WRONG - keeps duplicates
skills = ["Python", "python", "PYTHON"]

# ‚úÖ CORRECT - normalize and deduplicate
def _deduplicate_skills(self, skills: List[str]) -> List[str]:
    normalized = {}
    for skill in skills:
        key = skill.lower().strip()
        if key not in normalized:
            normalized[key] = skill.strip()
    return list(normalized.values())
```

## Requirements

1. Text Preprocessing
   - Remove excessive whitespace
   - Fix OCR/PDF extraction issues
   - Truncate if too long (max 20,000 chars)

2. Required vs Optional Fields
   - REQUIRED: name, email
   - IMPORTANT: phone, skills, experience
   - OPTIONAL: linkedin, github, projects

3. LLM Integration
   - Use Together AI (model: Mixtral-8x7B-Instruct-v0.1)
   - Temperature: 0.1 (low for accuracy)
   - Max tokens: 2000

4. Error Handling
   - Try/except for API calls
   - Fallback parsing if LLM fails
   - Validation errors with clear messages

5. Ethical Guardrails
   - Never fabricate data
   - Only extract what's in resume
   - Validate all extracted fields
```

**Step 2: Run Cline Generation**

```bash
# Generate agent from prompt
cline generate \
  --prompt cline/prompts/resume_parser.md \
  --output backend/agents/ \
  --config cline/cline.config.json
```

**What Happens:**
1. Cline reads the prompt file
2. Sends prompt + config to Claude 3 Opus
3. Claude generates Python code following the template
4. Cline saves code to `backend/agents/resume_parser.py`

**Step 3: Generated Code Structure**

Cline generates code following this structure:

```python
"""
Agent: ResumeParserAgent
Description: Parses unstructured resume text into structured data
Generated by: Cline CLI

Fixed Issues:
1. ‚úÖ Field validation for required fields
2. ‚úÖ Skill deduplication and normalization
3. ‚úÖ Date normalization
4. ‚úÖ Error handling with fallback parsing
"""

import json
import os
import re
from typing import Optional, List
from together import Together
from pydantic import BaseModel, EmailStr, field_validator
from models.schemas import ParsedResume, Experience, Education

class ResumeParserAgent:
    """
    Agent responsible for parsing resumes into structured format.
    
    This agent:
    ‚úÖ Extracts structured data from unstructured text
    ‚úÖ Validates required fields (name, email)
    ‚úÖ Normalizes dates and skills
    ‚úÖ Handles errors gracefully with fallback parsing
    
    This agent NEVER:
    ‚ùå Fabricates data not in the resume
    ‚ùå Accepts invalid email addresses
    ‚ùå Returns incomplete data without warnings
    """
    
    def __init__(self, api_key: Optional[str] = None):
        """Initialize Together AI client."""
        self.client = Together(
            api_key=api_key or os.getenv("TOGETHER_API_KEY")
        )
        self.model = "mistralai/Mixtral-8x7B-Instruct-v0.1"
    
    def parse(self, resume_text: str) -> ParsedResume:
        """
        Parse resume text into structured format.
        
        Args:
            resume_text: Raw resume text (up to 20,000 chars)
        
        Returns:
            ParsedResume: Structured resume data
        
        Raises:
            ValueError: If required fields are missing
        """
        # 1. Preprocess text
        cleaned_text = self._preprocess(resume_text)
        
        # 2. Call Together AI
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a precise resume parser."},
                    {"role": "user", "content": self._build_prompt(cleaned_text)}
                ],
                temperature=0.1,
                max_tokens=2000
            )
            
            # 3. Parse JSON response
            result_text = response.choices[0].message.content.strip()
            parsed_data = json.loads(self._clean_json(result_text))
            
            # 4. Validate and normalize
            parsed_resume = self._validate_and_normalize(parsed_data)
            
            return parsed_resume
            
        except Exception as e:
            # Fallback parsing if LLM fails
            logger.warning(f"LLM parsing failed: {e}, using fallback")
            return self._fallback_parse(cleaned_text)
    
    def _preprocess(self, text: str) -> str:
        """Clean resume text for parsing."""
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Fix common OCR/PDF issues
        text = text.replace('‚Ä¢', '-')
        text = text.replace('‚óè', '-')
        
        # Truncate if too long
        if len(text) > 20000:
            text = text[:20000] + "..."
        
        return text.strip()
    
    def _deduplicate_skills(self, skills: List[str]) -> List[str]:
        """Normalize and deduplicate skills."""
        normalized = {}
        for skill in skills:
            key = skill.lower().strip()
            if key not in normalized:
                normalized[key] = skill.strip()
        return list(normalized.values())
    
    def _validate_and_normalize(self, data: dict) -> ParsedResume:
        """Validate required fields and normalize data."""
        # Validate required fields
        if not data.get("name"):
            raise ValueError("Resume must have a name")
        if not data.get("email"):
            raise ValueError("Resume must have an email address")
        
        # Normalize skills
        if "skills" in data:
            data["skills"] = self._deduplicate_skills(data["skills"])
        
        # Normalize dates
        if "experience" in data:
            for exp in data["experience"]:
                if "duration" in exp:
                    exp["duration"] = self._normalize_date(exp["duration"])
        
        return ParsedResume(**data)
    
    def _fallback_parse(self, text: str) -> ParsedResume:
        """Fallback parsing if LLM fails."""
        # Extract basic info using regex
        email_match = re.search(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', text)
        email = email_match.group(0) if email_match else None
        
        if not email:
            raise ValueError("Could not extract email from resume")
        
        # Extract name (first line usually)
        lines = text.split('\n')
        name = lines[0].strip() if lines else "Unknown"
        
        return ParsedResume(
            name=name,
            email=email,
            skills=[],
            experience=[],
            warnings=["LLM parsing failed, using fallback extraction"]
        )


# CLI Support
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        with open(sys.argv[1], 'r') as f:
            resume_text = f.read()
        
        agent = ResumeParserAgent()
        result = agent.parse(resume_text)
        print(result.model_dump_json(indent=2))
    else:
        print("Usage: python resume_parser.py <resume.txt>")
```

**Key Features of Generated Code:**

1. **Structured Class:**
   - Clear docstrings explaining what agent does
   - Lists what it does ‚úÖ and doesn't do ‚ùå

2. **Error Handling:**
   - Try/except for API calls
   - Fallback parsing if LLM fails
   - Validation errors with clear messages

3. **Ethical Guardrails:**
   - Never fabricates data
   - Validates all extracted fields
   - Warns about missing important fields

4. **Integration:**
   - Uses Together AI (from config)
   - Follows Pydantic v2 patterns
   - Includes CLI support for testing

---

### **Cline Prompt Structure**

Every prompt in `cline/prompts/` follows this structure:

```markdown
# Cline Prompt: [Agent Name]

## Objective
Clear description of what the agent should do.

## Critical Issues to Fix
Known bugs from previous implementations with exact solutions.

### Issue #1: [Bug Name]
```python
# ‚ùå WRONG - broken code
...

# ‚úÖ CORRECT - fixed code
...
```

## Requirements
Detailed implementation requirements:
1. Feature 1
2. Feature 2
3. ...

## Safety & Ethics
Guardrails to prevent fabrication or unethical behavior.

## Data Flow
How input/output connects to other agents.

## Error Cases
What could go wrong and how to handle it.
```

**Example: `ats_scorer.md` Prompt**

```markdown
# Cline Prompt: ATS Scorer Agent

## Critical Issues to Fix

### Issue #1: Wrong Match Ratio Calculation
```python
# ‚ùå WRONG - mixes required with preferred
match_ratio = len(matching_skills) / len(jd.required_skills + jd.preferred_skills)

# ‚úÖ CORRECT - only count required skills
required_matches = [s for s in matching_skills if s.importance == "required"]
match_ratio = len(required_matches) / len(jd.required_skills)
```

### Issue #2: Issues Don't Affect Score
```python
# ‚ùå WRONG - issues identified but not penalized
issues = self._identify_issues(...)
final_score = base_score  # Issues ignored!

# ‚úÖ CORRECT - apply penalties
issue_penalty = sum(
    SEVERITY_PENALTIES.get(i.severity, 0) 
    for i in issues
)
final_score = base_score + issue_penalty  # Negative penalties
```

## Requirements

1. Component Scoring
   - Skill Match: 40% (required skills only)
   - Keywords: 20%
   - Formatting: 20%
   - Experience: 20%

2. Issue Penalties
   - Critical: -25 points
   - High: -10 points
   - Medium: -5 points

3. Bucket Determination
   - Strong: 80+ AND 70%+ required skills
   - Moderate: 60-79 AND 50%+ required skills
   - Weak: 40-59
   - Not ATS-Friendly: <40

## Safety & Ethics
- Never inflate scores artificially
- Issues must affect final score
- Be honest about gaps
```

**When Cline generates code from this prompt, it:**
- ‚úÖ Fixes the match ratio bug
- ‚úÖ Applies issue penalties correctly
- ‚úÖ Implements bucket logic accurately
- ‚úÖ Includes all safety guardrails

---

### **Cline Workflow in Development**

**1. Initial Agent Generation:**

```bash
# Generate all agents from prompts
cd cline

# Generate resume parser
cline generate --prompt prompts/resume_parser.md --output ../backend/agents/

# Generate JD analyzer
cline generate --prompt prompts/jd_analyzer.md --output ../backend/agents/

# Generate ATS scorer
cline generate --prompt prompts/ats_scorer.md --output ../backend/agents/

# ... etc for all agents
```

**2. Regenerating After Bug Fixes:**

```bash
# Update prompt with bug fix
vim prompts/ats_scorer.md
# Add new issue to "Critical Issues to Fix" section

# Regenerate agent
cline generate --prompt prompts/ats_scorer.md --output ../backend/agents/

# Verify changes
diff backend/agents/ats_scorer.py backend/agents/ats_scorer.py.bak
```

**3. Adding New Agents:**

```bash
# 1. Create new prompt
vim cline/prompts/new_agent.md
# Follow template structure

# 2. Add to config
vim cline/cline.config.json
# Add to "prompts" section

# 3. Generate agent
cline generate --prompt prompts/new_agent.md --output ../backend/agents/
```

---

### **Cline Integration with Project**

**1. Generated Agents in `backend/agents/`:**

All agents in `backend/agents/` were generated (or can be regenerated) using Cline:

```
backend/agents/
‚îú‚îÄ‚îÄ parse_resume.py          # Generated from resume_parser.md
‚îú‚îÄ‚îÄ analyze_jd.py            # Generated from jd_analyzer.md
‚îú‚îÄ‚îÄ gap_analysis.py          # Generated from gap_analysis.md
‚îú‚îÄ‚îÄ ats_scorer.py            # Generated from ats_scorer.md
‚îú‚îÄ‚îÄ skill_agent.py           # Generated from skill_agent.md
‚îú‚îÄ‚îÄ resume_rewrite.py        # Generated from resume_rewrite.md
‚îú‚îÄ‚îÄ cover_letter.py          # Generated from cover_letter.md
‚îî‚îÄ‚îÄ ...
```

**2. Import in `main.py`:**

```python
# Lines 34-46: Import agents (generated by Cline)
from agents import (
    ResumeParserAgent,      # Generated from resume_parser.md
    JDAnalyzerAgent,        # Generated from jd_analyzer.md
    GapAnalysisAgent,       # Generated from gap_analysis.md
    ATSScorerAgent,         # Generated from ats_scorer.md
    ResumeRewriteAgent,     # Generated from resume_rewrite.md
    CoverLetterAgent,       # Generated from cover_letter.md
    ...
)
```

**3. Usage in Pipeline:**

```python
# Lines 140-150: Initialize agents (generated by Cline)
resume_parser = ResumeParserAgent()      # Cline-generated
jd_analyzer = JDAnalyzerAgent()          # Cline-generated
gap_analyzer = GapAnalysisAgent()       # Cline-generated
ats_scorer = ATSScorerAgent()            # Cline-generated
resume_rewriter = ResumeRewriteAgent()    # Cline-generated
cover_letter_agent = CoverLetterAgent()  # Cline-generated
```

---

### **Cline vs Manual Coding**

**Without Cline (Manual):**
```python
# Developer writes from scratch
class ResumeParserAgent:
    def __init__(self):
        # ... write initialization
        pass
    
    def parse(self, text):
        # ... write parsing logic
        # ... write error handling
        # ... write validation
        # ... write fallback
        pass
```
**Time:** 2-4 hours per agent  
**Issues:** Inconsistent structure, missing error handling, bugs

**With Cline (Generated):**
```bash
# Developer creates prompt
vim cline/prompts/resume_parser.md

# Cline generates complete agent
cline generate --prompt prompts/resume_parser.md --output ../backend/agents/
```
**Time:** 10-15 minutes (prompt writing)  
**Benefits:** Consistent structure, error handling, bug fixes included

---

### **Cline Prompt Quality Standards**

Every prompt must include:

**1. Critical Issues to Fix:**
```markdown
### Issue #1: [Bug Name]
```python
# ‚ùå WRONG - broken code
...

# ‚úÖ CORRECT - fixed code
...
```
```

**2. Detailed Requirements:**
- Specific implementation guidance
- Not vague descriptions
- Code examples where helpful

**3. Safety & Ethics:**
- Guardrails to prevent fabrication
- Validation requirements
- Honesty constraints

**4. Data Flow:**
- Input/output interfaces
- How connects to other agents
- Pydantic model references

**5. Error Cases:**
- What could go wrong
- How to handle gracefully
- Fallback strategies

---

### **Cline and Kestra Integration**

Generated agents are automatically compatible with Kestra:

```yaml
# kestra/autoapplyai_pipeline.yaml

tasks:
  - id: parse_resume
    type: io.kestra.plugin.scripts.python.Commands
    commands:
      - cd backend && python -m agents.parse_resume
    # Cline-generated agents have CLI support
    # Can be run directly from command line
```

**Why This Works:**
- Cline generates CLI support (`if __name__ == "__main__"`)
- Agents can read from environment variables
- Agents can write output to files
- Perfect for Kestra task execution

---

### **Cline Configuration Deep Dive**

**Generator Settings:**

```json
"generator": {
  "model": "claude-3-opus",    // Best model for code generation
  "temperature": 0.2,          // Low = consistent output
  "maxTokens": 4000            // Enough for complete agent
}
```

**Why These Settings:**
- **Claude 3 Opus:** Best code generation quality
- **Temperature 0.2:** Consistent code structure across generations
- **Max Tokens 4000:** Complete agent class with all methods

**Templates:**

```json
"templates": {
  "agent": {
    "outputDir": "../backend/agents",
    "filePattern": "{name}.py",
    "includes": [
      "pydantic models",      // Always use Pydantic for validation
      "type hints",           // Full type annotations
      "docstrings",           // Google-style docstrings
      "error handling",       // Try/except blocks
      "CLI support"           // Command-line interface
    ]
  }
}
```

**What Gets Generated:**
- ‚úÖ Pydantic models for input/output
- ‚úÖ Type hints on all methods
- ‚úÖ Google-style docstrings
- ‚úÖ Error handling (try/except)
- ‚úÖ CLI support (`if __name__ == "__main__"`)

**Conventions:**

```json
"conventions": {
  "codeStyle": "black",              // Auto-format with Black
  "docstringFormat": "google",       // Google-style docstrings
  "importOrder": ["stdlib", "thirdparty", "local"],
  "typeHints": true,                 // Require type hints
  "pydanticV2": true                 // Use Pydantic v2
}
```

**Why These Conventions:**
- **Black:** Consistent code formatting
- **Google docstrings:** Standard Python documentation style
- **Type hints:** Better IDE support and error catching
- **Pydantic v2:** Latest validation features

---

### **Example: Generating ATS Scorer Agent**

**Step 1: Create Prompt (`cline/prompts/ats_scorer.md`)**

```markdown
# Cline Prompt: ATS Scorer Agent

## Critical Issues to Fix

### Issue #1: Wrong Match Ratio
# ‚ùå WRONG
match_ratio = len(matching) / len(required + preferred)

# ‚úÖ CORRECT
match_ratio = len(required_matches) / len(required)

## Requirements
1. Calculate component scores (skill, keyword, format, experience)
2. Apply issue penalties
3. Determine bucket (strong/moderate/weak/not_ats_friendly)
...

## Safety & Ethics
- Issues must affect score
- Never inflate scores
...
```

**Step 2: Run Cline**

```bash
cline generate \
  --prompt cline/prompts/ats_scorer.md \
  --output backend/agents/ \
  --config cline/cline.config.json
```

**Step 3: Generated Code (`backend/agents/ats_scorer.py`)**

```python
"""
Agent: ATSScorerAgent
Generated by: Cline CLI

Fixed Issues:
1. ‚úÖ Match ratio calculation (only required skills)
2. ‚úÖ Issue penalties applied to score
3. ‚úÖ Bucket determination logic
"""

class ATSScorerAgent:
    def score(self, resume, jd, gap_analysis) -> ATSScore:
        # ‚úÖ CORRECT implementation
        required_matches = [s for s in matching_skills 
                          if s.importance == "required"]
        match_ratio = len(required_matches) / len(jd.required_skills)
        
        # Calculate base score
        base_score = ...
        
        # Apply penalties
        issue_penalty = sum(...)
        final_score = base_score + issue_penalty  # ‚úÖ Issues affect score
        
        return ATSScore(...)
```

**Step 4: Use in Pipeline**

```python
# In main.py
ats_scorer = ATSScorerAgent()  # Cline-generated
ats_score = ats_scorer.score(parsed_resume, parsed_jd, gap_analysis)
```

---

### **Cline Benefits Summary**

**1. Speed:**
- Generate complete agent in seconds
- No need to write boilerplate
- Focus on prompt quality, not code

**2. Consistency:**
- All agents follow same structure
- Same error handling patterns
- Same documentation style

**3. Quality:**
- Includes bug fixes from prompts
- Best practices built-in
- Ethical guardrails enforced

**4. Maintainability:**
- Easy to regenerate with fixes
- Update prompt, regenerate code
- Version control prompts, not generated code

**5. Integration:**
- Works with Together AI (from config)
- Compatible with Kestra (CLI support)
- Uses Pydantic v2 (from config)

---

### **üí∞ Cline Cost Model (Important!)**

**Cline is NOT used at runtime. It's a development-time tool only.**

**Cost Breakdown:**

| Phase | Tool | Cost | Frequency |
|-------|------|------|-----------|
| **Development** | Cline (Claude 3 Opus) | $0.50-2.00 per agent | One-time or occasional |
| **Runtime** | Together AI | $0.01-0.05 per user | Every request |

**Example:**
- Generate 10 agents: $5-20 (one-time)
- Commit generated code to Git
- Deploy without Cline dependency
- Runtime uses Together AI only (no Cline calls)

**Verification:**
```bash
# Check: No Cline imports in runtime code
grep -r "cline" backend/agents/  # Should find nothing

# Check: Together AI is used
grep -r "Together(" backend/agents/  # Should find many
```

**Key Point:** Once code is generated and committed, Cline is not needed. Your runtime cost is Together AI, not Cline.

See `CLINE_COST_EXPLANATION.md` for detailed cost analysis.

---

### **Cline in the Development Workflow**

```
Developer Workflow:
1. Identify need for new agent
   ‚Üì
2. Create prompt in cline/prompts/
   - Define requirements
   - List critical issues to fix
   - Add safety guardrails
   ‚Üì
3. Run Cline generation
   cline generate --prompt prompts/new_agent.md
   ‚Üì
4. Review generated code
   - Check structure
   - Verify bug fixes
   - Test functionality
   ‚Üì
5. Use in pipeline
   from agents import NewAgent
   agent = NewAgent()
   result = agent.process(...)
   ‚Üì
6. If bugs found:
   - Update prompt with fix
   - Regenerate agent
   - Test again
```

---

### **Cline vs Other Code Generation Tools**

| Feature | Cline | GitHub Copilot | ChatGPT |
|---------|-------|----------------|---------|
| **Purpose** | Agent generation | Code completion | General chat |
| **Output** | Complete agent class | Code snippets | Varies |
| **Structure** | Consistent templates | Ad-hoc | Ad-hoc |
| **Bug Fixes** | From prompts | Manual | Manual |
| **Integration** | Config-based | Editor-based | Manual copy/paste |
| **Best For** | Structured agents | Quick snippets | Exploratory coding |

**Why Cline for This Project:**
- ‚úÖ Generates complete, production-ready agents
- ‚úÖ Includes bug fixes and best practices
- ‚úÖ Consistent structure across all agents
- ‚úÖ Easy to regenerate with updates
- ‚úÖ Integrates with project config (Together AI, Kestra)

---

## üìÑ Main Pipeline (`main.py`)

### **Lines 1-32: Imports and Setup**

```python
# Lines 1-4: Module docstring and standard imports
import os          # Environment variables
import time        # Performance timing
import uuid        # Unique workflow IDs
import logging     # Structured logging

# Lines 14-22: Logging Configuration
logging.basicConfig(
    level=logging.INFO,  # Log level
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]  # Console output
)
logger = logging.getLogger("AutoApplyAI")  # Named logger

# Lines 24-26: Environment Variables
env_path = Path(__file__).resolve().parent.parent / ".env"
load_dotenv(env_path)  # Load from root .env file

# Lines 28-32: FastAPI and Pydantic imports
from fastapi import FastAPI, File, UploadFile, ...
from pydantic import BaseModel  # Data validation
import io  # For file streaming
```

**What This Does:**
- Sets up logging for debugging
- Loads environment variables (API keys, config)
- Imports FastAPI framework for REST API
- Imports Pydantic for data validation

---

### **Lines 34-56: Agent Imports**

```python
# Lines 34-46: Import all agents
from agents import (
    ResumeParserAgent,      # Parses resume text ‚Üí structured data
    JDAnalyzerAgent,        # Analyzes job description
    GapAnalysisAgent,       # Compares resume vs JD
    SkillAgent,             # Enhances and standardizes skills
    OumiATSClassifier,      # Fine-tuned ATS classifier ‚≠ê
    ATSScorerAgent,         # Detailed ATS scoring
    ResumeRewriteAgent,     # Optimizes resume content
    CoverLetterAgent,       # Generates cover letter
    ExplanationAgent,        # Explains analysis
    ProjectRecommendationAgent  # Suggests projects
)

# Lines 48-52: Import data models
from models.schemas import (
    ProcessRequest, ProcessResponse, AutoApplyResult,
    ParsedResume, ParsedJobDescription
)

# Lines 54-56: Import utilities
from utils.file_handlers import extract_text_from_file
from utils.pdf_generator import generate_resume_pdf, ...
```

**What This Does:**
- Imports all AI agents (each uses Together AI)
- Imports data models (Pydantic schemas)
- Imports utility functions (file parsing, PDF generation)

---

### **Lines 59-112: FastAPI App Setup**

```python
# Lines 59-60: Store for async results
processing_results = {}  # In-memory cache for async jobs

# Lines 63-72: Application Lifespan
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler"""
    # Startup
    logger.info("üöÄ AutoApply AI Backend Starting...")
    logger.info(f"üìã Multi-agent pipeline ready")
    logger.info(f"üîë Together API Key: {'‚úì Set' if os.getenv('TOGETHER_API_KEY') else '‚úó Missing'}")
    yield  # App runs here
    # Shutdown
    logger.info("üëã AutoApply AI Backend Shutting Down...")

# Lines 75-80: Create FastAPI App
app = FastAPI(
    title="AutoApply AI",
    description="AI-powered resume optimization and ATS scoring system",
    version="1.0.0",
    lifespan=lifespan  # Use lifespan handler
)

# Lines 83-89: CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins (configure for production)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Lines 93-111: Request Logging Middleware
@app.middleware("http")
async def log_requests(request: Request, call_next):
    """Log all incoming requests"""
    start_time = time.time()
    logger.info(f"üì• {request.method} {request.url.path}")
    
    try:
        response = await call_next(request)
        process_time = time.time() - start_time
        logger.info(f"üì§ {request.method} {request.url.path} - {response.status_code} ({process_time:.2f}s)")
        return response
    except Exception as e:
        logger.error(f"‚ùå {request.method} {request.url.path} - Error: {str(e)}")
        raise
```

**What This Does:**
- Sets up FastAPI application with lifecycle management
- Configures CORS (allows frontend to call API)
- Adds request logging middleware (tracks all API calls)

---

### **Lines 114-118: Request Model**

```python
class TextProcessRequest(BaseModel):
    """Request model for text-based processing"""
    resume_text: str      # Raw resume text
    job_description: str  # Job description text
```

**What This Does:**
- Defines request structure (validated by Pydantic)
- Ensures both fields are strings and required

---

## üîÑ Core Pipeline Function (`run_pipeline`)

### **Lines 120-240: The Main Pipeline**

```python
def run_pipeline(resume_text: str, jd_text: str) -> AutoApplyResult:
    """
    Execute the full multi-agent pipeline
    
    Pipeline Flow (orchestrated by Kestra):
    1. Parse Resume
    2. Analyze Job Description
    3. Gap Analysis
    4. Skill Enhancement
    5. OUMI ATS Classification (fine-tuned model) ‚≠ê
    6. Together AI ATS Scoring
    7. Resume Rewrite
    8. Cover Letter Generation
    9. Explanation
    10. Project Recommendations
    """
    
    # Line 137: Start timing
    start_time = time.time()
    logger.info(f"üöÄ Starting pipeline - Resume: {len(resume_text)} chars, JD: {len(jd_text)} chars")
```

**What This Does:**
- Main orchestration function
- Takes raw resume and JD text
- Returns complete analysis result
- Tracks execution time

---

### **Lines 140-150: Initialize All Agents**

```python
    # Initialize agents (partially generated by Cline CLI)
    resume_parser = ResumeParserAgent()        # Uses Together AI
    jd_analyzer = JDAnalyzerAgent()           # Uses Together AI
    gap_analyzer = GapAnalysisAgent()         # Uses Together AI + SkillMatcher
    skill_agent = SkillAgent()                 # Uses Together AI
    oumi_classifier = OumiATSClassifier()     # ‚≠ê Uses OUMI (rule-based fallback)
    ats_scorer = ATSScorerAgent()             # Uses Together AI
    resume_rewriter = ResumeRewriteAgent()     # Uses Together AI
    cover_letter_agent = CoverLetterAgent()   # Uses Together AI
    explanation_agent = ExplanationAgent()     # Uses Together AI
    project_agent = ProjectRecommendationAgent()  # Uses Together AI
```

**What This Does:**
- Creates instances of all agents
- Each agent initializes its own Together AI client
- OUMI classifier uses rule-based system (can use fine-tuned model if available)

---

### **Step 1: Parse Resume (Lines 152-155)**

```python
    # Step 1: Parse Resume
    logger.info("üìÑ Step 1: Parsing resume...")
    parsed_resume = resume_parser.parse(resume_text)
    logger.info(f"   ‚úì Found {len(parsed_resume.skills)} skills, {len(parsed_resume.experience)} experiences")
```

**What Happens Inside `ResumeParserAgent.parse()`:**

```python
# In agents/parse_resume.py

def parse(self, resume_text: str) -> ParsedResume:
    # 1. Clean and truncate text (max 20,000 chars)
    resume_text = self._clean_and_truncate(resume_text)
    
    # 2. Create prompt for Together AI
    prompt = f"""You are an expert resume parser...
    Return JSON with: name, email, skills, experience, etc.
    Resume Text: {resume_text}"""
    
    # 3. Call Together AI API
    response = self.client.chat.completions.create(
        model="mistralai/Mixtral-8x7B-Instruct-v0.1",  # Together AI model
        messages=[
            {"role": "system", "content": "You are a precise resume parser."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1,  # Low temperature for accuracy
        max_tokens=2000
    )
    
    # 4. Parse JSON response
    result_text = response.choices[0].message.content.strip()
    parsed_data = json.loads(result_text)
    
    # 5. Return structured ParsedResume object
    return ParsedResume(**parsed_data)
```

**Together AI Usage:**
- **Model:** `mistralai/Mixtral-8x7B-Instruct-v0.1`
- **Purpose:** Extract structured data from unstructured resume text
- **Input:** Raw resume text (up to 20,000 chars)
- **Output:** JSON with name, email, skills, experience, etc.

---

### **Step 2: Analyze Job Description (Lines 157-160)**

```python
    # Step 2: Analyze Job Description
    logger.info("üìã Step 2: Analyzing job description...")
    parsed_jd = jd_analyzer.analyze(jd_text)
    logger.info(f"   ‚úì Role: {parsed_jd.role}, Required skills: {len(parsed_jd.required_skills)}")
```

**What Happens Inside `JDAnalyzerAgent.analyze()`:**

```python
# In agents/analyze_jd.py

def analyze(self, jd_text: str) -> ParsedJobDescription:
    # 1. Clean and truncate (max 15,000 chars, keep LAST part)
    jd_text = self._clean_and_truncate(jd_text)  # Keeps requirements section
    
    # 2. Create prompt for Together AI
    prompt = f"""You are an expert job description analyzer...
    Extract:
    - required_skills: Skills marked as "required" or "must-have"
    - preferred_skills: Skills marked as "preferred" or "nice-to-have"
    - keywords: Technical keywords for ATS matching
    - seniority: entry/junior/mid/senior/lead
    - experience_years: "3-5 years" or null
    
    Job Description: {jd_text}"""
    
    # 3. Call Together AI
    response = self.client.chat.completions.create(
        model="mistralai/Mixtral-8x7B-Instruct-v0.1",
        messages=[...],
        temperature=0.1,
        max_tokens=2000
    )
    
    # 4. Parse and validate
    parsed_data = json.loads(response.choices[0].message.content)
    parsed_data = self._validate_and_normalize(parsed_data)
    
    # 5. Return ParsedJobDescription
    return ParsedJobDescription(**parsed_data)
```

**Together AI Usage:**
- **Model:** `mistralai/Mixtral-8x7B-Instruct-v0.1`
- **Purpose:** Extract job requirements from JD text
- **Input:** Job description text (up to 15,000 chars)
- **Output:** Structured JD with required_skills, preferred_skills, keywords, seniority

---

### **Step 3: Gap Analysis (Lines 162-165)**

```python
    # Step 3: Gap Analysis
    logger.info("üîç Step 3: Performing gap analysis...")
    gap_analysis = gap_analyzer.analyze(parsed_resume, parsed_jd)
    logger.info(f"   ‚úì Match: {gap_analysis.overall_match_percentage}%")
```

**What Happens Inside `GapAnalysisAgent.analyze()`:**

```python
# In agents/gap_analysis.py

def analyze(self, resume: ParsedResume, jd: ParsedJobDescription) -> GapAnalysis:
    # Uses SkillMatcher (semantic matching) + Together AI for insights
    
    matching_skills = []
    missing_skills = []
    
    # 1. Match required skills using SkillMatcher
    for skill_name in jd.required_skills:
        match_result = self.skill_matcher.match(
            skill_name,           # JD skill: "Kubernetes"
            resume.skills,        # Resume skills: ["K8s", "Docker", ...]
            resume.raw_text       # Full resume text for context
        )
        
        # SkillMatcher uses 5 strategies:
        # 1. Exact match: "Kubernetes" == "Kubernetes"
        # 2. Equivalence: "K8s" == "Kubernetes" (from ontology)
        # 3. Text appearance: "Kubernetes" found in description
        # 4. Partial match: "Kubernetes" in "Kubernetes orchestration"
        # 5. Semantic similarity: Uses sentence-transformers embeddings
        
        if match_result.confidence >= 0.7:  # 70% confidence threshold
            matching_skills.append(SkillMatch(...))
        else:
            missing_skills.append(SkillMatch(...))
    
    # 2. Match preferred skills (same process)
    # 3. Match keywords (same process)
    # 4. Match tools (same process)
    
    # 5. Generate insights using Together AI
    insights_prompt = f"""Based on gap analysis:
    Matching: {matching_skills}
    Missing: {missing_skills}
    
    Generate actionable insights..."""
    
    insights = self._generate_insights_with_llm(insights_prompt)
    
    # 6. Return GapAnalysis
    return GapAnalysis(
        matching_skills=matching_skills,
        missing_skills=missing_skills,
        overall_match_percentage=...,
        insights=insights
    )
```

**Together AI + SkillMatcher Usage:**
- **SkillMatcher:** Semantic matching (sentence-transformers)
- **Together AI:** Generates insights and recommendations
- **Output:** Detailed gap analysis with match confidence scores

---

### **Step 4: Skill Enhancement (Lines 167-174)**

```python
    # Step 4: Skill Enhancement (ethical)
    logger.info("‚ú® Step 4: Enhancing skills...")
    enhanced_skills = skill_agent.enhance_skills(parsed_resume)
    
    # Update resume with standardized skills if available
    if enhanced_skills.get("standardized_skills"):
        parsed_resume.skills = enhanced_skills["standardized_skills"]
    logger.info(f"   ‚úì Skills enhanced")
```

**What Happens Inside `SkillAgent.enhance_skills()`:**

```python
# In agents/skill_agent.py

def enhance_skills(self, resume: ParsedResume) -> Dict:
    # 1. Standardize skill names using SKILL_SYNONYMS
    # "JS" ‚Üí "JavaScript", "K8s" ‚Üí "Kubernetes"
    standardized = self._standardize_skills(resume.skills)
    
    # 2. Extract skills from experience descriptions
    # Uses Together AI to find skills mentioned in bullets
    extracted_skills = self._extract_skills_from_experience(resume)
    
    # 3. Validate no fabrication
    # Checks that all skills are actually in resume
    validated = self._validate_no_fabrication(standardized, resume)
    
    # 4. Categorize skills
    categorized = self._categorize_skills(validated)
    
    return {
        "standardized_skills": standardized,
        "categorized_skills": categorized,
        "extracted_skills": extracted_skills
    }
```

**Together AI Usage:**
- **Purpose:** Extract skills from experience descriptions
- **Example:** "Built REST APIs with FastAPI" ‚Üí extracts "FastAPI", "REST API"

---

### **Step 5: OUMI ATS Classification (Lines 176-179)** ‚≠ê

```python
    # Step 5: OUMI ATS Classification (provides signals for Together AI)
    logger.info("üéØ Step 5: OUMI ATS classification...")
    oumi_result = oumi_classifier.classify(parsed_resume, parsed_jd)
    logger.info(f"   ‚úì ATS Bucket: {oumi_result.get('ats_bucket', 'N/A')}")
```

**What Happens Inside `OumiATSClassifier.classify()`:**

```python
# In agents/oumi_ats_classifier.py

def classify(self, resume: ParsedResume, jd: ParsedJobDescription) -> Dict:
    """
    OUMI Classification Process:
    
    Option 1: If fine-tuned model available
    - Load OUMI model from disk
    - Run inference on resume+JD
    - Return bucket classification
    
    Option 2: Rule-based fallback (current implementation)
    - Use SkillMatcher for skill matching
    - Calculate scores using rules
    - Return bucket + confidence
    """
    
    # 1. Match skills using SkillMatcher
    required_matches = self._match_skills(
        jd.required_skills,
        resume.skills,
        resume.raw_text,
        importance="required"
    )
    
    # 2. Calculate component scores
    skill_score = self._calculate_skill_match_score(resume, jd)
    keyword_score = self._calculate_keyword_score(resume, jd)
    formatting_score = self._calculate_formatting_score(resume)
    experience_score = self._calculate_experience_score(resume, jd)
    
    # 3. Weighted overall score
    overall = (
        skill_score * 0.60 +      # Required skills: 60%
        keyword_score * 0.15 +    # Keywords: 15%
        formatting_score * 0.15 +  # Formatting: 15%
        experience_score * 0.10    # Experience: 10%
    )
    
    # 4. Determine bucket
    if overall >= 80 and skill_score >= 70:
        bucket = "strong"
    elif overall >= 60 and skill_score >= 50:
        bucket = "moderate"
    elif overall >= 40:
        bucket = "weak"
    else:
        bucket = "not_ats_friendly"
    
    # 5. Calculate confidence
    confidence = self._calculate_confidence(overall, required_matches)
    
    return {
        "ats_bucket": bucket,
        "ats_score": int(overall),
        "confidence": confidence,
        "signals": {
            "skill_match": skill_score,
            "keyword_match": keyword_score,
            "formatting": formatting_score,
            "experience": experience_score
        }
    }
```

**OUMI Usage:**
- **Current:** Rule-based classification (no fine-tuned model yet)
- **Future:** Can load fine-tuned model from `oumi/sample_dataset/`
- **Purpose:** Fast, deterministic ATS bucket classification
- **Output:** Bucket (strong/moderate/weak/not_ats_friendly) + confidence

**How to Use Fine-Tuned OUMI:**
```python
# If you train OUMI model:
oumi_model = load_model("./models/finetuned_ats")
bucket = oumi_model.predict(resume, jd)  # Returns "strong", "moderate", etc.
```

---

### **Step 6: Together AI ATS Scoring (Lines 181-199)**

```python
    # Step 6: Together AI ATS Scoring (enhanced with OUMI signals)
    logger.info("üìä Step 6: ATS scoring...")
    ats_score = ats_scorer.score(parsed_resume, parsed_jd, gap_analysis)
    logger.info(f"   ‚úì ATS Score: {ats_score.overall_score}/100 ({ats_score.bucket.value})")
    logger.info(f"   üìä Components: skill={ats_score.skill_match_score}, keyword={ats_score.keyword_score}, format={ats_score.formatting_score}, exp={ats_score.experience_alignment_score}")
    
    # Debug: Calculate expected score
    expected = (
        ats_score.skill_match_score * 0.40 +
        ats_score.keyword_score * 0.20 +
        ats_score.formatting_score * 0.20 +
        ats_score.experience_alignment_score * 0.20
    )
    logger.info(f"   üìê Expected (before penalties): {expected:.1f}")
    
    if ats_score.issues:
        critical = len([i for i in ats_score.issues if i.severity == "critical"])
        high = len([i for i in ats_score.issues if i.severity == "high"])
        logger.info(f"   ‚ö†Ô∏è Issues: {critical} critical, {high} high")
```

**What Happens Inside `ATSScorerAgent.score()`:**

```python
# In agents/ats_scorer.py

def score(self, resume, jd, gap_analysis) -> ATSScore:
    """
    Detailed ATS Scoring Process:
    
    1. Calculate component scores (rule-based)
    2. Apply issue penalties
    3. Use Together AI for recommendations
    4. Return comprehensive score
    """
    
    # 1. Calculate component scores (rule-based, not LLM)
    skill_score = self._calculate_skill_score(gap_analysis, jd)
    keyword_score = self._calculate_keyword_score(gap_analysis)
    formatting_score = self._calculate_formatting_score(resume)
    experience_score = self._calculate_experience_score(resume, jd)
    
    # 2. Weighted base score
    base_score = (
        skill_score * 0.40 +      # Required skills: 40%
        keyword_score * 0.20 +    # Keywords: 20%
        formatting_score * 0.20 +  # Formatting: 20%
        experience_score * 0.20    # Experience: 20%
    )
    
    # 3. Identify issues
    issues = self._identify_issues(resume, jd, gap_analysis)
    
    # 4. Apply penalties
    issue_penalty = sum(
        self.SEVERITY_PENALTIES.get(i.severity, 0) 
        for i in issues
    )
    
    # 5. Generate recommendations using Together AI
    recommendations_prompt = f"""Based on ATS score {base_score}:
    Issues: {issues}
    Gaps: {gap_analysis.missing_skills}
    
    Generate actionable recommendations..."""
    
    recommendations = self._get_recommendations_with_llm(recommendations_prompt)
    
    # 6. Final score
    final_score = max(0, min(100, base_score + issue_penalty))
    
    return ATSScore(
        overall_score=final_score,
        bucket=self._determine_bucket(final_score, ...),
        skill_match_score=skill_score,
        keyword_score=keyword_score,
        formatting_score=formatting_score,
        experience_alignment_score=experience_score,
        issues=issues,
        recommendations=recommendations
    )
```

**Together AI Usage:**
- **Purpose:** Generate personalized recommendations
- **Input:** ATS score, issues, gaps
- **Output:** Actionable recommendations (e.g., "Add Kubernetes to skills section")

**Note:** OUMI signals are NOT directly used here, but OUMI bucket can inform scoring logic.

---

### **Step 7: Resume Rewrite (Lines 201-206)**

```python
    # Step 7: Resume Rewrite
    logger.info("‚úçÔ∏è Step 7: Rewriting resume...")
    rewritten_resume = resume_rewriter.rewrite(
        parsed_resume, parsed_jd, gap_analysis, ats_score
    )
    logger.info("   ‚úì Resume rewritten")
```

**What Happens Inside `ResumeRewriteAgent.rewrite()`:**

```python
# In agents/resume_rewrite.py

def rewrite(self, resume, jd, gap_analysis, ats_score) -> RewrittenResume:
    """
    Honest Resume Optimization Process:
    
    1. Generate summary (uses Together AI)
    2. Improve bullets (uses Together AI)
    3. Reorder skills (rule-based)
    4. Enhance experience (extract skills from descriptions)
    5. Determine section order (rule-based)
    """
    
    # 1. Generate summary using Together AI
    summary_prompt = f"""Write professional summary for {jd.role}.
    Matching skills: {gap_analysis.matching_skills}
    Missing skills: {gap_analysis.missing_skills}
    ATS Score: {ats_score.overall_score}
    
    Guidelines:
    - Lead with matching skills
    - Do NOT claim missing skills
    - Be honest about experience level"""
    
    summary = self.client.chat.completions.create(
        model="mistralai/Mixtral-8x7B-Instruct-v0.1",
        messages=[...],
        temperature=0.3  # Lower for consistency
    )
    
    # 2. Improve bullets using Together AI
    bullets_prompt = f"""Improve these resume bullets HONESTLY.
    Do NOT add numbers/metrics unless in original.
    Do NOT claim skills not present.
    
    Bullets: {resume.experience[0].description}"""
    
    improved_bullets = self.client.chat.completions.create(...)
    
    # 3. Reorder skills (rule-based, no LLM)
    reordered_skills = self._reorder_skills(resume, jd, gap_analysis)
    
    # 4. Enhance experience (extract skills from descriptions)
    enhanced_experience = self._enhance_experience(resume, jd, gap_analysis)
    
    return RewrittenResume(
        summary=summary,
        improved_bullets=improved_bullets,
        reordered_skills=reordered_skills,
        enhanced_experience=enhanced_experience,
        ...
    )
```

**Together AI Usage:**
- **Summary Generation:** Creates targeted professional summary
- **Bullet Improvement:** Improves action verbs, clarity
- **Model:** `mistralai/Mixtral-8x7B-Instruct-v0.1`
- **Temperature:** 0.3 (lower for consistency)

---

### **Step 8-10: Cover Letter, Explanation, Projects (Lines 208-225)**

```python
    # Step 8: Cover Letter Generation
    logger.info("üìù Step 8: Generating cover letter...")
    cover_letter = cover_letter_agent.generate(
        parsed_resume, parsed_jd, gap_analysis
    )
    
    # Step 9: Explanation
    logger.info("üí° Step 9: Generating explanation...")
    explanation = explanation_agent.explain(
        parsed_resume, parsed_jd, gap_analysis, ats_score
    )
    
    # Step 10: Project Recommendations
    logger.info("üìö Step 10: Generating project recommendations...")
    project_recommendations = project_agent.recommend(gap_analysis)
```

**All use Together AI:**
- **Cover Letter:** Generates personalized cover letter
- **Explanation:** Explains ATS score and gaps
- **Projects:** Suggests learning projects

---

### **Lines 227-240: Return Result**

```python
    processing_time = time.time() - start_time
    logger.info(f"‚úÖ Pipeline complete! Total time: {processing_time:.2f}s")
    
    return AutoApplyResult(
        parsed_resume=parsed_resume,
        parsed_jd=parsed_jd,
        gap_analysis=gap_analysis,
        ats_score=ats_score,
        rewritten_resume=rewritten_resume,
        cover_letter=cover_letter,
        explanation=explanation,
        project_recommendations=project_recommendations,
        processing_time=round(processing_time, 2)
    )
```

**What This Does:**
- Calculates total processing time
- Returns complete result object
- All data is Pydantic-validated

---

## üéØ Together AI Integration Details

### **How Together AI is Used**

**1. Initialization (in each agent):**

```python
# Example from ResumeParserAgent
def __init__(self, api_key: Optional[str] = None):
    self.client = Together(
        api_key=api_key or os.getenv("TOGETHER_API_KEY")
    )
    self.model = "mistralai/Mixtral-8x7B-Instruct-v0.1"
```

**2. API Call Pattern:**

```python
response = self.client.chat.completions.create(
    model=self.model,                    # Together AI model
    messages=[
        {"role": "system", "content": "You are an expert..."},
        {"role": "user", "content": prompt}
    ],
    temperature=0.1,                     # Low for accuracy
    max_tokens=2000                       # Response length limit
)

result = response.choices[0].message.content
```

**3. Models Used:**

| Agent | Model | Purpose | Temperature |
|-------|-------|---------|-------------|
| Resume Parser | Mixtral-8x7B | Extract structured data | 0.1 |
| JD Analyzer | Mixtral-8x7B | Extract requirements | 0.1 |
| Gap Analysis | Mixtral-8x7B | Generate insights | 0.3 |
| Resume Rewrite | Mixtral-8x7B | Optimize content | 0.3 |
| Cover Letter | Mixtral-8x7B | Generate letter | 0.7 |
| Explanation | Mixtral-8x7B | Explain analysis | 0.5 |
| Projects | Mixtral-8x7B | Suggest projects | 0.7 |

**4. Token Management:**

```python
# Each agent truncates input to fit context window
MAX_RESUME_CHARS = 20000  # ~5000 tokens
MAX_JD_CHARS = 15000     # ~3750 tokens

# Together AI context window: 32,768 tokens
# We use ~8,000 tokens per call (safe margin)
```

---

## üéØ OUMI Integration Details

### **Current Implementation (Rule-Based)**

```python
# In agents/oumi_ats_classifier.py

class OumiATSClassifier:
    def __init__(self, model_path: Optional[str] = None):
        self.model_path = model_path  # Path to fine-tuned model (if available)
        
        # Fallback: Use SkillMatcher for semantic matching
        self.matcher = SkillMatcher() if MATCHER_AVAILABLE else None
    
    def classify(self, resume, jd) -> Dict:
        # Option 1: Use fine-tuned OUMI model (if available)
        if self.model_path and os.path.exists(self.model_path):
            return self._oumi_inference(resume, jd)
        
        # Option 2: Rule-based fallback (current)
        return self._rule_based_classify(resume, jd)
```

**Rule-Based Classification:**

```python
def _rule_based_classify(self, resume, jd) -> Dict:
    # 1. Match skills using SkillMatcher (semantic embeddings)
    required_matches = self._match_skills(jd.required_skills, resume.skills, resume.raw_text)
    
    # 2. Calculate scores
    skill_score = self._calculate_skill_match_score(resume, jd)
    keyword_score = self._calculate_keyword_score(resume, jd)
    formatting_score = self._calculate_formatting_score(resume)
    experience_score = self._calculate_experience_score(resume, jd)
    
    # 3. Weighted overall
    overall = (
        skill_score * 0.60 +
        keyword_score * 0.15 +
        formatting_score * 0.15 +
        experience_score * 0.10
    )
    
    # 4. Determine bucket
    bucket = self._determine_bucket(overall, required_matches)
    
    return {
        "ats_bucket": bucket,
        "ats_score": int(overall),
        "confidence": self._calculate_confidence(overall, required_matches),
        "signals": {...}
    }
```

**Future: Fine-Tuned OUMI Model**

```python
def _oumi_inference(self, resume, jd) -> Dict:
    """
    If you train OUMI model from oumi/sample_dataset/train.jsonl:
    
    1. Train model:
       oumi train --dataset sample_dataset --model llama-3-8b --output models/finetuned_ats
    
    2. Load model:
       model = load_model("models/finetuned_ats")
    
    3. Run inference:
       bucket = model.predict(resume, jd)  # Returns "strong", "moderate", etc.
    
    4. Return result:
       return {"ats_bucket": bucket, "confidence": 0.95, ...}
    """
    pass
```

**OUMI vs Together AI ATS Scoring:**

| Aspect | OUMI | Together AI ATS Scorer |
|--------|------|----------------------|
| **Purpose** | Fast bucket classification | Detailed scoring |
| **Output** | Bucket (strong/moderate/weak) | Score 0-100 + components |
| **Speed** | Fast (rule-based) | Slower (LLM calls) |
| **Accuracy** | Good (semantic matching) | Excellent (LLM reasoning) |
| **Use Case** | Quick classification | Detailed analysis |

---

## üîÑ Kestra Orchestration

### **How Kestra Orchestrates the Pipeline**

**Kestra Pipeline Structure:**

```yaml
# kestra/autoapplyai_pipeline.yaml

id: autoapplyai_pipeline
namespace: autoapplyai

inputs:
  - id: resume_text
    type: STRING
  - id: job_description
    type: STRING

tasks:
  # Task 1: Validate Inputs
  - id: validate_inputs
    type: io.kestra.plugin.scripts.python.Commands
    commands:
      - python3 << 'EOF'
        # Validation logic
        EOF
    outputFiles:
      - resume.txt
      - jd.txt
  
  # Task 2: Parse Resume (uses Together AI)
  - id: parse_resume
    type: io.kestra.plugin.scripts.python.Commands
    dependsOn:
      - validate_inputs
    inputFiles:
      resume.txt: "{{ outputs.validate_inputs.outputFiles['resume.txt'] }}"
    env:
      RESUME_INPUT: "resume.txt"
      RESUME_OUTPUT: "parsed_resume.json"
    commands:
      - pip install -q together python-dotenv pydantic
      - cd backend && python -m tasks.parse_resume_task
    outputFiles:
      - parsed_resume.json
  
  # Task 3: Analyze JD (parallel with parse_resume)
  - id: analyze_jd
    type: io.kestra.plugin.scripts.python.Commands
    dependsOn:
      - validate_inputs  # Same dependency, runs in parallel
    commands:
      - cd backend && python -m tasks.analyze_jd_task
    outputFiles:
      - parsed_jd.json
  
  # Task 4: Gap Analysis (waits for both parse_resume AND analyze_jd)
  - id: gap_analysis
    type: io.kestra.plugin.scripts.python.Commands
    dependsOn:
      - parse_resume
      - analyze_jd
    inputFiles:
      parsed_resume.json: "{{ outputs.parse_resume.outputFiles['parsed_resume.json'] }}"
      parsed_jd.json: "{{ outputs.analyze_jd.outputFiles['parsed_jd.json'] }}"
    commands:
      - cd backend && python -m tasks.gap_analysis_task
  
  # Task 5: OUMI Classification
  - id: oumi_classification
    type: io.kestra.plugin.scripts.python.Commands
    dependsOn:
      - gap_analysis
    commands:
      - cd backend && python3 << 'EOF'
        from agents.oumi_ats_classifier import OumiATSClassifier
        classifier = OumiATSClassifier()
        result = classifier.classify(resume, jd)
        # Save to oumi_classification.json
        EOF
  
  # Task 6: ATS Scoring (uses OUMI signals)
  - id: ats_scoring
    type: io.kestra.plugin.scripts.python.Commands
    dependsOn:
      - oumi_classification
    inputFiles:
      oumi_classification.json: "{{ outputs.oumi_classification.outputFiles['oumi_classification.json'] }}"
    env:
      OUMI_INPUT: "oumi_classification.json"  # Pass OUMI result
    commands:
      - cd backend && python -m tasks.ats_scoring_task
```

**Kestra Features Used:**

1. **Dependencies:**
   ```yaml
   dependsOn:
     - parse_resume
     - analyze_jd
   ```
   - Ensures tasks run in correct order
   - `parse_resume` and `analyze_jd` run in parallel (both depend on `validate_inputs`)

2. **File Passing:**
   ```yaml
   inputFiles:
     parsed_resume.json: "{{ outputs.parse_resume.outputFiles['parsed_resume.json'] }}"
   ```
   - Kestra passes files between tasks
   - Uses template syntax `{{ outputs.task.outputFiles['file'] }}`

3. **Environment Variables:**
   ```yaml
   env:
     RESUME_INPUT: "resume.txt"
     RESUME_OUTPUT: "parsed_resume.json"
   ```
   - Tasks read from environment variables
   - Makes code portable

4. **Error Handling:**
   ```yaml
   timeout: PT5M        # 5 minute timeout
   retry:
     maxAttempt: 3      # Retry up to 3 times
     delay: PT10S       # 10 second delay between retries
   ```

5. **Conditional Execution:**
   ```yaml
   - id: check_score
     # Checks if score > 35
   
   - id: resume_rewrite
     dependsOn:
       - check_score
     # Only runs if score is high enough
   ```

---

## üìä Complete Data Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  USER REQUEST                                               ‚îÇ
‚îÇ  POST /process                                               ‚îÇ
‚îÇ  { resume_text: "...", job_description: "..." }             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FASTAPI BACKEND (main.py)                                  ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  run_pipeline(resume_text, jd_text)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 1: Parse Resume                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ ResumeParserAgent                                   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Uses Together AI (Mixtral-8x7B)                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Input: Raw resume text                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Output: ParsedResume (structured)               ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 2: Analyze Job Description                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ JDAnalyzerAgent                                    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Uses Together AI (Mixtral-8x7B)                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Input: JD text                                  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Output: ParsedJobDescription                    ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 3: Gap Analysis                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ GapAnalysisAgent                                   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Uses SkillMatcher (semantic embeddings)        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Uses Together AI (for insights)                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Input: ParsedResume + ParsedJobDescription     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Output: GapAnalysis (matches, gaps, insights)   ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 4: Skill Enhancement                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ SkillAgent                                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Uses Together AI (extract skills from text)     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Uses SKILL_SYNONYMS (standardize names)         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Output: Enhanced skills list                     ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 5: OUMI ATS Classification ‚≠ê                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ OumiATSClassifier                                  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Uses SkillMatcher (semantic matching)          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Uses rule-based scoring                         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - (Future: Fine-tuned OUMI model)                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Output: { ats_bucket, ats_score, confidence }  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 6: Together AI ATS Scoring                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ ATSScorerAgent                                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Uses rule-based component scoring               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Uses Together AI (for recommendations)         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Input: ParsedResume + ParsedJD + GapAnalysis   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Output: ATSScore (0-100, bucket, issues)       ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 7: Resume Rewrite                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ ResumeRewriteAgent                                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Uses Together AI (summary, bullets)            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Uses rule-based (skill reordering, experience) ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  - Output: RewrittenResume (optimized)            ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 8-10: Cover Letter, Explanation, Projects            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ All use Together AI                                ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RETURN AutoApplyResult                                      ‚îÇ
‚îÇ  {                                                           ‚îÇ
‚îÇ    parsed_resume, parsed_jd, gap_analysis,                  ‚îÇ
‚îÇ    ats_score, rewritten_resume, cover_letter,               ‚îÇ
‚îÇ    explanation, project_recommendations                     ‚îÇ
‚îÇ  }                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîç Agent-by-Agent Breakdown

### **1. ResumeParserAgent**

**Together AI Usage:**
```python
# Line 19: Initialize Together client
self.client = Together(api_key=os.getenv("TOGETHER_API_KEY"))

# Line 20: Set model
self.model = "mistralai/Mixtral-8x7B-Instruct-v0.1"

# Lines 100-120: Call Together AI
response = self.client.chat.completions.create(
    model=self.model,
    messages=[
        {"role": "system", "content": "You are a precise resume parser."},
        {"role": "user", "content": prompt}
    ],
    temperature=0.1,  # Low for accuracy
    max_tokens=2000
)
```

**What It Does:**
- Takes unstructured resume text
- Uses Together AI to extract structured data
- Returns `ParsedResume` object

**Kestra Integration:**
- Task: `parse_resume`
- Runs: After `validate_inputs`
- Output: `parsed_resume.json`

---

### **2. JDAnalyzerAgent**

**Together AI Usage:**
- Same pattern as ResumeParserAgent
- Extracts: role, required_skills, preferred_skills, keywords, seniority

**Kestra Integration:**
- Task: `analyze_jd`
- Runs: In parallel with `parse_resume` (both depend on `validate_inputs`)
- Output: `parsed_jd.json`

---

### **3. GapAnalysisAgent**

**Hybrid Approach:**
```python
# Uses SkillMatcher (semantic embeddings)
match_result = self.skill_matcher.match(skill_name, resume.skills, resume.raw_text)

# Uses Together AI for insights
insights = self._generate_insights_with_llm(gap_analysis)
```

**Kestra Integration:**
- Task: `gap_analysis`
- Runs: After both `parse_resume` AND `analyze_jd` complete
- Input: `parsed_resume.json` + `parsed_jd.json`
- Output: `gap_analysis.json`

---

### **4. OumiATSClassifier** ‚≠ê

**Current Implementation:**
```python
# Rule-based (no fine-tuned model yet)
def classify(self, resume, jd):
    # Uses SkillMatcher for semantic matching
    matches = self._match_skills(jd.required_skills, resume.skills, resume.raw_text)
    
    # Calculates scores using rules
    score = self._calculate_scores(matches, resume, jd)
    
    # Determines bucket
    bucket = self._determine_bucket(score)
    
    return {"ats_bucket": bucket, "ats_score": score, ...}
```

**Future: Fine-Tuned Model:**
```python
# If model trained:
def classify(self, resume, jd):
    if self.model_path:
        # Load fine-tuned OUMI model
        model = load_model(self.model_path)
        # Run inference
        bucket = model.predict(resume, jd)
        return {"ats_bucket": bucket, ...}
    else:
        # Fallback to rule-based
        return self._rule_based_classify(resume, jd)
```

**Kestra Integration:**
- Task: `oumi_classification`
- Runs: After `gap_analysis`
- Output: `oumi_classification.json`
- Used by: `ats_scoring` task (as input signal)

---

### **5. ATSScorerAgent**

**Hybrid Approach:**
```python
# Rule-based scoring
skill_score = self._calculate_skill_score(gap_analysis, jd)  # Rule-based
keyword_score = self._calculate_keyword_score(gap_analysis)  # Rule-based
formatting_score = self._calculate_formatting_score(resume)  # Rule-based
experience_score = self._calculate_experience_score(resume, jd)  # Rule-based

# Together AI for recommendations
recommendations = self._get_recommendations_with_llm(...)  # Together AI
```

**Kestra Integration:**
- Task: `ats_scoring`
- Runs: After `oumi_classification`
- Input: `parsed_resume.json` + `parsed_jd.json` + `gap_analysis.json` + `oumi_classification.json`
- Output: `ats_score.json`

---

### **6. ResumeRewriteAgent**

**Together AI Usage:**
```python
# Summary generation
summary = self.client.chat.completions.create(
    model="mistralai/Mixtral-8x7B-Instruct-v0.1",
    messages=[...],
    temperature=0.3
)

# Bullet improvement
improved_bullets = self.client.chat.completions.create(...)
```

**Rule-Based:**
- Skill reordering (rule-based)
- Experience enhancement (extract from descriptions)
- Section ordering (rule-based)

**Kestra Integration:**
- Task: `resume_rewrite`
- Runs: After `ats_scoring` (conditional on score > 35)
- Output: `rewritten_resume.json`

---

## üîÑ Kestra vs FastAPI: Two Execution Paths

### **Path 1: FastAPI Direct (Current)**

```
User ‚Üí FastAPI /process ‚Üí run_pipeline() ‚Üí Agents ‚Üí Result
```

**When Used:**
- Direct API calls from frontend
- Synchronous processing
- Immediate results

**Flow:**
1. User calls `POST /process`
2. FastAPI calls `run_pipeline()`
3. All agents run sequentially in Python
4. Result returned immediately

---

### **Path 2: Kestra Orchestration (Alternative)**

```
User ‚Üí Kestra Webhook ‚Üí Tasks ‚Üí Agents ‚Üí Result
```

**When Used:**
- Large-scale processing
- Need retries/timeouts
- Need observability
- Need conditional logic

**Flow:**
1. User triggers Kestra workflow (via webhook)
2. Kestra executes tasks in order
3. Each task runs agent code
4. Results compiled at end

**Kestra Advantages:**
- ‚úÖ Retry logic (auto-retry failed tasks)
- ‚úÖ Timeouts (prevent hanging)
- ‚úÖ Observability (see each step)
- ‚úÖ Conditional execution (skip if score too low)
- ‚úÖ Parallel execution (parse_resume + analyze_jd)

---

## üìä Together AI API Call Pattern

**Every Agent Follows This Pattern:**

```python
class SomeAgent:
    def __init__(self):
        # 1. Initialize Together client
        self.client = Together(api_key=os.getenv("TOGETHER_API_KEY"))
        self.model = "mistralai/Mixtral-8x7B-Instruct-v0.1"
    
    def process(self, input_data):
        # 2. Prepare prompt
        prompt = f"""You are an expert...
        Input: {input_data}
        Return JSON: {...}"""
        
        # 3. Call Together AI
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "You are..."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,  # Low for structured tasks
            max_tokens=2000
        )
        
        # 4. Parse response
        result_text = response.choices[0].message.content.strip()
        
        # 5. Clean JSON (remove markdown)
        if result_text.startswith("```json"):
            result_text = result_text[7:]
        if result_text.endswith("```"):
            result_text = result_text[:-3]
        
        # 6. Parse and return
        data = json.loads(result_text)
        return SomeModel(**data)
```

**Key Points:**
- All agents use same Together AI model
- Temperature varies by task (0.1 for parsing, 0.7 for creative)
- All responses are JSON
- All have error handling and fallbacks

---

## üéØ OUMI Integration Flow

**Current (Rule-Based):**

```
Resume + JD
    ‚Üì
OumiATSClassifier.classify()
    ‚Üì
SkillMatcher.match()  # Semantic matching
    ‚Üì
Calculate scores (rule-based)
    ‚Üì
Determine bucket (strong/moderate/weak/not_ats_friendly)
    ‚Üì
Return { ats_bucket, ats_score, confidence, signals }
```

**Future (Fine-Tuned Model):**

```
Resume + JD
    ‚Üì
OumiATSClassifier.classify()
    ‚Üì
Load fine-tuned OUMI model
    ‚Üì
Run inference
    ‚Üì
Return { ats_bucket, ats_score, confidence }
```

**How to Train OUMI:**

```bash
# 1. Prepare dataset (already done: 650 examples)
cd oumi/sample_dataset

# 2. Train model
oumi train \
  --dataset-train train.jsonl \
  --dataset-validation val.jsonl \
  --model llama-3-8b \
  --task classification \
  --target-field bucket \
  --epochs 5 \
  --output ../models/finetuned_ats

# 3. Update OumiATSClassifier
classifier = OumiATSClassifier(model_path="./models/finetuned_ats")
```

---

## üîÑ Complete Request Flow Example

**User Request:**
```json
POST /process
{
  "resume_text": "John Doe\nSoftware Engineer...",
  "job_description": "Backend Developer\nRequired: Python, FastAPI..."
}
```

**Step-by-Step Execution:**

```
1. FastAPI receives request
   ‚Üì
2. run_pipeline() called
   ‚Üì
3. ResumeParserAgent.parse()
   - Together AI call #1
   - Returns: ParsedResume
   ‚Üì
4. JDAnalyzerAgent.analyze()
   - Together AI call #2
   - Returns: ParsedJobDescription
   ‚Üì
5. GapAnalysisAgent.analyze()
   - SkillMatcher (semantic matching)
   - Together AI call #3 (insights)
   - Returns: GapAnalysis
   ‚Üì
6. SkillAgent.enhance_skills()
   - Together AI call #4 (extract skills)
   - Returns: Enhanced skills
   ‚Üì
7. OumiATSClassifier.classify()
   - SkillMatcher (semantic matching)
   - Rule-based scoring
   - Returns: { ats_bucket: "moderate", ats_score: 65 }
   ‚Üì
8. ATSScorerAgent.score()
   - Rule-based component scores
   - Together AI call #5 (recommendations)
   - Returns: ATSScore (75/100)
   ‚Üì
9. ResumeRewriteAgent.rewrite()
   - Together AI call #6 (summary)
   - Together AI call #7 (bullets)
   - Rule-based (skill reordering)
   - Returns: RewrittenResume
   ‚Üì
10. CoverLetterAgent.generate()
    - Together AI call #8
    - Returns: CoverLetter
   ‚Üì
11. ExplanationAgent.explain()
    - Together AI call #9
    - Returns: Explanation
   ‚Üì
12. ProjectRecommendationAgent.recommend()
    - Together AI call #10
    - Returns: ProjectRecommendations
   ‚Üì
13. Return AutoApplyResult
    - All results combined
    - Processing time: 15.3s
```

**Total Together AI Calls: 10**
**Total Processing Time: ~15-20 seconds**

---

## üéØ Key Takeaways

### **Cline CLI:**
- **Used for:** Auto-generating agent code from prompts
- **Model:** Claude 3 Opus (for code generation)
- **Output:** Production-ready agent classes with bug fixes
- **Purpose:** Speed, consistency, and quality in agent development

### **Together AI:**
- **Used for:** All LLM tasks (parsing, analysis, generation)
- **Model:** `mistralai/Mixtral-8x7B-Instruct-v0.1`
- **Calls:** ~10 per pipeline run
- **Purpose:** Intelligence layer (understanding, generation)

### **OUMI:**
- **Used for:** Fast ATS bucket classification
- **Current:** Rule-based (semantic matching)
- **Future:** Fine-tuned model (if trained)
- **Purpose:** Quick classification signal

### **Kestra:**
- **Used for:** Workflow orchestration (alternative to FastAPI)
- **Features:** Retries, timeouts, dependencies, conditional logic
- **Purpose:** Production-grade pipeline management

### **SkillMatcher:**
- **Used for:** Semantic skill matching
- **Technology:** sentence-transformers embeddings
- **Purpose:** Intelligent skill matching (handles aliases, variations)

---

## üìö Summary

**The system uses a four-tier architecture:**

1. **Cline (Code Generation):** Generates agent code from prompts, ensures consistency and bug fixes
2. **Kestra (Orchestration):** Manages workflow, retries, dependencies
3. **FastAPI (Application):** REST API, business logic, agent orchestration
4. **AI Agents (Intelligence):** Together AI for LLM tasks, OUMI for classification, SkillMatcher for matching

**Cline generates:**
- All agent classes in `backend/agents/`
- Consistent structure and error handling
- Bug fixes from prompt templates
- Ethical guardrails and validation

**Together AI powers:**
- Resume parsing
- JD analysis
- Gap insights
- Resume rewriting
- Cover letter generation
- Explanations
- Project recommendations

**OUMI provides:**
- Fast ATS bucket classification
- Semantic skill matching
- Confidence scores

**Kestra enables:**
- Production-grade orchestration
- Error handling
- Observability
- Conditional execution

**All working together to optimize resumes honestly and effectively!** üéØ

