"""
AutoApply AI - FastAPI Backend
Multi-agent system for resume optimization and ATS scoring
"""
import os
import time
import uuid
import logging
from pathlib import Path
from typing import Optional
from contextlib import asynccontextmanager
from dotenv import load_dotenv

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("AutoApplyAI")

# Load environment variables from root .env file
env_path = Path(__file__).resolve().parent.parent / ".env"
load_dotenv(env_path)

from fastapi import FastAPI, File, UploadFile, Form, HTTPException, BackgroundTasks, Request, Depends, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, Field, field_validator
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
import io
import json
from typing import Callable, Optional
from queue import Queue
import threading

# Import agents (partially generated by Cline CLI)
from agents import (
    ResumeParserAgent,
    JDAnalyzerAgent,
    GapAnalysisAgent,
    SkillAgent,
    OumiATSClassifier,  # Fine-tuned ATS model
    ATSScorerAgent,
    ResumeRewriteAgent,
    CoverLetterAgent,
    ExplanationAgent,
    ProjectRecommendationAgent
)

# Import models
from models.schemas import (
    ProcessRequest, ProcessResponse, AutoApplyResult,
    ParsedResume, ParsedJobDescription
)

# Import utilities
from utils.file_handlers import extract_text_from_file
from utils.pdf_generator import generate_resume_pdf, generate_cover_letter_pdf, generate_resume_word


# Store for async processing results with TTL-based cleanup
# SECURITY: Prevents memory leak by auto-cleaning old results
from datetime import datetime, timedelta
import asyncio

processing_results = {}
RESULT_TTL = timedelta(hours=24)  # Keep results for 24 hours

# Store for progress queues (workflow_id -> Queue)
progress_queues: dict[str, Queue] = {}


async def cleanup_old_results():
    """Periodically clean up old processing results to prevent memory leaks"""
    while True:
        try:
            current_time = datetime.now()
            expired_keys = [
                key for key, value in processing_results.items()
                if isinstance(value, dict) and "timestamp" in value
                and current_time - value["timestamp"] > RESULT_TTL
            ]
            for key in expired_keys:
                del processing_results[key]
                logger.info(f"üßπ Cleaned up expired result: {key}")
            
            # Also clean up results older than TTL without timestamp (legacy)
            if len(processing_results) > 1000:  # Safety limit
                # Remove oldest 100 entries if we exceed limit
                keys_to_remove = list(processing_results.keys())[:100]
                for key in keys_to_remove:
                    del processing_results[key]
                logger.warning(f"üßπ Cleaned up {len(keys_to_remove)} results (memory limit)")
        except Exception as e:
            logger.error(f"Error in cleanup task: {e}")
        
        await asyncio.sleep(3600)  # Run every hour


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler"""
    # Startup
    logger.info("üöÄ AutoApply AI Backend Starting...")
    
    # SECURITY: Validate required environment variables
    required_vars = ["TOGETHER_API_KEY"]
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    if missing_vars:
        error_msg = f"‚ùå CRITICAL: Missing required environment variables: {', '.join(missing_vars)}"
        logger.error(error_msg)
        raise RuntimeError(error_msg)
    
    logger.info(f"üìã Multi-agent pipeline ready")
    logger.info(f"üîë Together API Key: ‚úì Set")
    logger.info(f"üåê CORS Origins: {ALLOWED_ORIGINS}")
    
    # Start cleanup task
    cleanup_task = asyncio.create_task(cleanup_old_results())
    logger.info("üßπ Started result cleanup task")
    
    yield
    
    # Shutdown
    cleanup_task.cancel()
    try:
        await cleanup_task
    except asyncio.CancelledError:
        pass
    # Shutdown
    logger.info("üëã AutoApply AI Backend Shutting Down...")


app = FastAPI(
    title="AutoApply AI",
    description="AI-powered resume optimization and ATS scoring system",
    version="1.0.0",
    lifespan=lifespan
)

# SECURITY: Rate limiting
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# CORS configuration - SECURITY: Restrict origins in production
ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "").split(",")
# Remove empty strings and default to localhost for development
ALLOWED_ORIGINS = [origin.strip() for origin in ALLOWED_ORIGINS if origin.strip()]
if not ALLOWED_ORIGINS or os.getenv("ENVIRONMENT") != "production":
    # Development: Allow localhost
    ALLOWED_ORIGINS = ["http://localhost:3000", "http://127.0.0.1:3000"]
    logger.warning("‚ö†Ô∏è CORS: Using development origins. Set ALLOWED_ORIGINS for production!")

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["Content-Type", "Authorization", "X-API-Key"],
)


# SECURITY: API Key authentication
API_KEY_HEADER = "X-API-Key"
API_KEY = os.getenv("API_KEY")  # Set this in production

async def verify_api_key(
    x_api_key: str = Header(None, alias=API_KEY_HEADER),
    authorization: str = Header(None)
):
    """
    Verify API key for protected endpoints
    
    SECURITY: In production, require API key or JWT token
    For development, allow if API_KEY env var is not set
    """
    # Skip auth check if API_KEY not configured (development mode)
    if not API_KEY:
        if os.getenv("ENVIRONMENT") == "production":
            raise HTTPException(
                status_code=500,
                detail="API authentication not configured"
            )
        return True  # Development mode - allow
    
    # Check API key header
    if x_api_key and x_api_key == API_KEY:
        return True
    
    # Check Authorization header (Bearer token)
    if authorization and authorization.startswith("Bearer "):
        token = authorization.split(" ")[1]
        if token == API_KEY:
            return True
    
    raise HTTPException(
        status_code=401,
        detail="Invalid or missing API key"
    )


# Request logging middleware
@app.middleware("http")
async def log_requests(request: Request, call_next):
    """Log all incoming requests"""
    start_time = time.time()
    
    # Log request (don't log sensitive paths)
    if "/health" not in request.url.path:
        logger.info(f"üì• {request.method} {request.url.path}")
    
    try:
        response = await call_next(request)
        
        # Log response
        process_time = time.time() - start_time
        if "/health" not in request.url.path:
            logger.info(f"üì§ {request.method} {request.url.path} - {response.status_code} ({process_time:.2f}s)")
        
        return response
    except HTTPException:
        raise
    except Exception as e:
        # SECURITY: Don't expose internal error details to users
        logger.error(f"‚ùå {request.method} {request.url.path} - Error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="An internal error occurred. Please try again later."
        )


class TextProcessRequest(BaseModel):
    """Request model for text-based processing"""
    resume_text: str = Field(..., min_length=10, max_length=500000)  # SECURITY: Input validation
    job_description: str = Field(..., min_length=10, max_length=500000)
    
    @field_validator('resume_text', 'job_description')
    @classmethod
    def validate_text(cls, v: str) -> str:
        """Validate and sanitize text input"""
        if not v or not v.strip():
            raise ValueError("Text cannot be empty")
        # Remove null bytes and other dangerous characters
        v = v.replace('\x00', '')
        if len(v) > 500000:
            raise ValueError("Text too long. Maximum 500,000 characters")
        return v.strip()


def run_pipeline(resume_text: str, jd_text: str, progress_callback: Optional[Callable] = None) -> AutoApplyResult:
    """
    Execute the full multi-agent pipeline
    
    Pipeline Flow (orchestrated by Kestra):
    1. Parse Resume
    2. Analyze Job Description
    3. Gap Analysis
    4. Skill Enhancement
    5. OUMI ATS Classification (fine-tuned model)
    6. Together AI ATS Scoring
    7. Resume Rewrite
    8. Cover Letter Generation
    9. Explanation
    10. Project Recommendations
    
    Args:
        resume_text: Resume text to process
        jd_text: Job description text
        progress_callback: Optional callback function to emit progress updates
    """
    
    def emit_progress(step: int, step_name: str, status: str, message: str = "", details: dict = None):
        """Helper to emit progress updates"""
        if progress_callback:
            progress_callback({
                "step": step,
                "step_name": step_name,
                "status": status,  # "pending", "processing", "completed", "failed"
                "message": message,
                "details": details or {},
                "timestamp": datetime.now().isoformat()
            })
    
    start_time = time.time()
    logger.info(f"üöÄ Starting pipeline - Resume: {len(resume_text)} chars, JD: {len(jd_text)} chars")
    emit_progress(0, "Initialization", "processing", "Starting process...")
    
    # Initialize agents (partially generated by Cline CLI)
    resume_parser = ResumeParserAgent()
    jd_analyzer = JDAnalyzerAgent()
    gap_analyzer = GapAnalysisAgent()
    skill_agent = SkillAgent()
    oumi_classifier = OumiATSClassifier()  # Fine-tuned ATS model
    ats_scorer = ATSScorerAgent()
    resume_rewriter = ResumeRewriteAgent()
    cover_letter_agent = CoverLetterAgent()
    explanation_agent = ExplanationAgent()
    project_agent = ProjectRecommendationAgent()
    
    # Step 1: Parse Resume
    logger.info("üìÑ Step 1: Parsing resume...")
    emit_progress(1, "Parsing Resume", "processing", "Parsing resume...")
    parsed_resume = resume_parser.parse(resume_text)
    logger.info(f"   ‚úì Found {len(parsed_resume.skills)} skills, {len(parsed_resume.experience)} experiences")
    emit_progress(1, "Parsing Resume", "completed", f"Resume parsed successfully.", {"skills_count": len(parsed_resume.skills), "experience_count": len(parsed_resume.experience)})
    
    # Step 2: Analyze Job Description
    logger.info("üìã Step 2: Analyzing job description...")
    emit_progress(2, "Analyzing JD", "processing", "Analyzing job description...")
    parsed_jd = jd_analyzer.analyze(jd_text)
    logger.info(f"   ‚úì Role: {parsed_jd.role}, Required skills: {len(parsed_jd.required_skills)}")
    emit_progress(2, "Analyzing JD", "completed", f"Job Description Analysis Complete.", {"role": parsed_jd.role, "required_skills": parsed_jd.required_skills[:10]})
    
    # Step 3: Gap Analysis
    logger.info("üîç Step 3: Performing gap analysis...")
    emit_progress(3, "Skill Matching", "processing", "Comparing candidate skills against requirements...")
    gap_analysis = gap_analyzer.analyze(parsed_resume, parsed_jd)
    logger.info(f"   ‚úì Match: {gap_analysis.overall_match_percentage}%")
    emit_progress(3, "Skill Matching", "completed", f"Skill matching complete. Match: {gap_analysis.overall_match_percentage}%", {"match_percentage": gap_analysis.overall_match_percentage, "matching_skills": gap_analysis.matching_skills[:10]})
    
    # Step 4: Skill Enhancement (ethical)
    logger.info("‚ú® Step 4: Enhancing skills...")
    emit_progress(4, "Skill Enhancement", "processing", "Enhancing skills...")
    enhanced_skills = skill_agent.enhance_skills(parsed_resume)
    
    # Update resume with standardized skills if available
    if enhanced_skills.get("standardized_skills"):
        parsed_resume.skills = enhanced_skills["standardized_skills"]
    logger.info(f"   ‚úì Skills enhanced")
    emit_progress(4, "Skill Enhancement", "completed", "Skills enhanced")
    
    # Step 5: OUMI ATS Classification (provides signals for Together AI)
    logger.info("üéØ Step 5: OUMI ATS classification...")
    emit_progress(5, "ATS Scoring (Oumi)", "processing", "Running ATS classification...")
    oumi_result = oumi_classifier.classify(parsed_resume, parsed_jd)
    logger.info(f"   ‚úì ATS Bucket: {oumi_result.get('ats_bucket', 'N/A')}")
    emit_progress(5, "ATS Scoring (Oumi)", "completed", f"ATS Classification complete. Bucket: {oumi_result.get('ats_bucket', 'N/A')}", {"ats_bucket": oumi_result.get('ats_bucket', 'N/A')})
    
    # Step 6: Together AI ATS Scoring (enhanced with OUMI signals)
    logger.info("üìä Step 6: ATS scoring...")
    emit_progress(6, "ATS Scoring", "processing", "Calculating ATS score...")
    ats_score = ats_scorer.score(parsed_resume, parsed_jd, gap_analysis)
    logger.info(f"   ‚úì ATS Score: {ats_score.overall_score}/100 ({ats_score.bucket.value})")
    logger.info(f"   üìä Components: skill={ats_score.skill_match_score}, keyword={ats_score.keyword_score}, format={ats_score.formatting_score}, exp={ats_score.experience_alignment_score}")
    
    # Debug: Calculate expected score
    expected = (
        ats_score.skill_match_score * 0.40 +
        ats_score.keyword_score * 0.20 +
        ats_score.formatting_score * 0.20 +
        ats_score.experience_alignment_score * 0.20
    )
    logger.info(f"   üìê Expected (before penalties): {expected:.1f}")
    
    if ats_score.issues:
        critical = len([i for i in ats_score.issues if i.severity == "critical"])
        high = len([i for i in ats_score.issues if i.severity == "high"])
        logger.info(f"   ‚ö†Ô∏è Issues: {critical} critical, {high} high")
    
    emit_progress(6, "ATS Scoring", "completed", f"ATS Score: {ats_score.overall_score}/100", {
        "overall_score": ats_score.overall_score,
        "bucket": ats_score.bucket.value,
        "skill_match": ats_score.skill_match_score,
        "keyword": ats_score.keyword_score,
        "formatting": ats_score.formatting_score,
        "experience": ats_score.experience_alignment_score
    })
    
    # Step 7: Resume Rewrite
    logger.info("‚úçÔ∏è Step 7: Rewriting resume...")
    emit_progress(7, "AI Rewrite", "processing", "Rewriting resume with AI...")
    rewritten_resume = resume_rewriter.rewrite(
        parsed_resume, parsed_jd, gap_analysis, ats_score
    )
    logger.info("   ‚úì Resume rewritten")
    emit_progress(7, "AI Rewrite", "completed", "Resume rewritten successfully")
    
    # Step 8: Cover Letter Generation
    logger.info("üìù Step 8: Generating cover letter...")
    emit_progress(8, "Cover Letter", "processing", "Generating cover letter...")
    cover_letter = cover_letter_agent.generate(
        parsed_resume, parsed_jd, gap_analysis
    )
    logger.info(f"   ‚úì Cover letter: {cover_letter.word_count} words")
    emit_progress(8, "Cover Letter", "completed", f"Cover letter generated ({cover_letter.word_count} words)", {"word_count": cover_letter.word_count})
    
    # Step 9: Explanation
    logger.info("üí° Step 9: Generating explanation...")
    emit_progress(9, "Explanation", "processing", "Generating explanation...")
    explanation = explanation_agent.explain(
        parsed_resume, parsed_jd, gap_analysis, ats_score
    )
    logger.info("   ‚úì Explanation generated")
    emit_progress(9, "Explanation", "completed", "Explanation generated")
    
    # Step 10: Project Recommendations
    logger.info("üìö Step 10: Generating project recommendations...")
    emit_progress(10, "Project Recommendations", "processing", "Generating project recommendations...")
    project_recommendations = project_agent.recommend(gap_analysis)
    logger.info(f"   ‚úì {len(project_recommendations.recommended_projects)} projects recommended")
    emit_progress(10, "Project Recommendations", "completed", f"{len(project_recommendations.recommended_projects)} projects recommended", {"project_count": len(project_recommendations.recommended_projects)})
    
    processing_time = time.time() - start_time
    logger.info(f"‚úÖ Pipeline complete! Total time: {processing_time:.2f}s")
    emit_progress(11, "Complete", "completed", f"Processing complete! Total time: {processing_time:.2f}s", {"processing_time": round(processing_time, 2)})
    
    return AutoApplyResult(
        parsed_resume=parsed_resume,
        parsed_jd=parsed_jd,
        gap_analysis=gap_analysis,
        ats_score=ats_score,
        rewritten_resume=rewritten_resume,
        cover_letter=cover_letter,
        explanation=explanation,
        project_recommendations=project_recommendations,
        processing_time=round(processing_time, 2)
    )


def process_async(workflow_id: str, resume_text: str, jd_text: str):
    """Background task for async processing"""
    logger.info(f"üîÑ Async processing started: {workflow_id}")
    try:
        result = run_pipeline(resume_text, jd_text)
        processing_results[workflow_id] = {
            "status": "completed",
            "result": result,
            "timestamp": datetime.now()  # Add timestamp for TTL cleanup
        }
        logger.info(f"‚úÖ Async processing completed: {workflow_id}")
    except Exception as e:
        logger.error(f"‚ùå Async processing failed: {workflow_id} - {str(e)}")
        processing_results[workflow_id] = {
            "status": "failed",
            "error": str(e),
            "timestamp": datetime.now()
        }


@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "service": "AutoApply AI",
        "version": "1.0.0",
        "agents": [
            "ResumeParser",
            "JDAnalyzer",
            "GapAnalysis",
            "SkillAgent",
            "ATSScorer",
            "ResumeRewriter",
            "CoverLetter",
            "Explanation",
            "ProjectRecommendation"
        ]
    }


@app.get("/health")
async def health_check():
    """
    Detailed health check endpoint
    
    SECURITY: Provides system status without sensitive information
    """
    health_status = {
        "status": "healthy",
        "service": "AutoApply AI",
        "version": "1.0.0",
        "checks": {
            "api_key_configured": bool(API_KEY),
            "together_api_key": bool(os.getenv("TOGETHER_API_KEY")),
            "cors_configured": len(ALLOWED_ORIGINS) > 0,
            "processing_results_count": len(processing_results)
        }
    }
    
    # Check if any critical components are missing
    if not os.getenv("TOGETHER_API_KEY"):
        health_status["status"] = "degraded"
        health_status["checks"]["together_api_key"] = False
    
    return health_status


@app.post("/process", response_model=ProcessResponse)
@limiter.limit("10/hour")  # SECURITY: Rate limiting
async def process_resume(
    request: Request,
    text_request: TextProcessRequest,
    _: bool = Depends(verify_api_key)  # SECURITY: Require API key
):
    """
    Process resume and job description (synchronous)
    
    This endpoint runs the full multi-agent pipeline and returns results.
    """
    logger.info(f"üì• Process request received - Resume: {len(text_request.resume_text)} chars, JD: {len(text_request.job_description)} chars")
    
    try:
        result = run_pipeline(text_request.resume_text, text_request.job_description)
        logger.info("‚úÖ Process request completed successfully")
        
        return ProcessResponse(
            success=True,
            result=result
        )
    
    except Exception as e:
        logger.error(f"‚ùå Process request failed: {str(e)}", exc_info=True)
        return ProcessResponse(
            success=False,
            error=str(e)
        )


@app.post("/process/stream")
@limiter.limit("10/hour")  # SECURITY: Rate limiting
async def process_resume_stream(
    request: Request,
    text_request: TextProcessRequest,
    _: bool = Depends(verify_api_key)  # SECURITY: Require API key
):
    """
    Process resume and job description with Server-Sent Events (SSE) for real-time progress
    
    Returns a stream of progress updates and final result.
    """
    workflow_id = str(uuid.uuid4())
    progress_queue = Queue()
    progress_queues[workflow_id] = progress_queue
    
    logger.info(f"üì• Stream process request received - Workflow ID: {workflow_id}")
    
    def progress_callback(update: dict):
        """Callback to emit progress updates"""
        progress_queue.put(update)
    
    async def generate():
        """Generator function for SSE stream"""
        try:
            # Start processing in background thread
            import concurrent.futures
            executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
            future = executor.submit(run_pipeline, text_request.resume_text, text_request.job_description, progress_callback)
            
            result_data = None
            error_data = None
            
            # Monitor the future and stream progress
            while True:
                # Check if processing is complete
                if future.done():
                    try:
                        result_data = future.result()
                        # Send final result
                        yield f"data: {json.dumps({'type': 'result', 'workflow_id': workflow_id, 'result': result_data.model_dump()})}\n\n"
                        break
                    except Exception as e:
                        error_data = str(e)
                        yield f"data: {json.dumps({'type': 'error', 'workflow_id': workflow_id, 'error': error_data})}\n\n"
                        break
                
                # Stream progress updates
                try:
                    while not progress_queue.empty():
                        update = progress_queue.get(timeout=0.1)
                        yield f"data: {json.dumps({'type': 'progress', 'workflow_id': workflow_id, **update})}\n\n"
                except:
                    pass
                
                # Small delay to prevent busy waiting
                await asyncio.sleep(0.1)
            
            # Cleanup
            if workflow_id in progress_queues:
                del progress_queues[workflow_id]
            executor.shutdown(wait=False)
            
        except Exception as e:
            logger.error(f"‚ùå Stream processing error: {str(e)}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'workflow_id': workflow_id, 'error': str(e)})}\n\n"
            if workflow_id in progress_queues:
                del progress_queues[workflow_id]
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@app.post("/process/async")
@limiter.limit("10/hour")  # SECURITY: Rate limiting
async def process_resume_async(
    request: Request,
    text_request: TextProcessRequest,
    background_tasks: BackgroundTasks,
    _: bool = Depends(verify_api_key)  # SECURITY: Require API key
):
    """
    Process resume and job description (asynchronous)
    
    Returns a workflow ID that can be used to check status.
    """
    workflow_id = str(uuid.uuid4())
    
    processing_results[workflow_id] = {
        "status": "processing",
        "timestamp": datetime.now()
    }
    
    background_tasks.add_task(
        process_async,
        workflow_id,
        text_request.resume_text,
        text_request.job_description
    )
    
    return {
        "workflow_id": workflow_id,
        "status": "processing",
        "message": "Resume processing started. Use /status/{workflow_id} to check progress."
    }


@app.get("/status/{workflow_id}")
async def check_status(workflow_id: str):
    """Check the status of an async processing job"""
    
    if workflow_id not in processing_results:
        raise HTTPException(status_code=404, detail="Workflow not found")
    
    result = processing_results[workflow_id]
    
    if result["status"] == "completed":
        return ProcessResponse(
            success=True,
            workflow_id=workflow_id,
            result=result["result"]
        )
    elif result["status"] == "failed":
        return ProcessResponse(
            success=False,
            workflow_id=workflow_id,
            error=result.get("error", "Unknown error")
        )
    else:
        return {"workflow_id": workflow_id, "status": "processing"}


@app.post("/parse-file")
async def parse_file(file: UploadFile = File(...)):
    """
    Parse a file (PDF, DOCX, TXT) and return extracted text.
    Used by frontend to properly extract resume text.
    """
    try:
        content = await file.read()
        # SECURITY: Validate file before processing
        from utils.file_handlers import validate_file
        validate_file(content, file.filename, file.content_type)
        
        text = extract_text_from_file(content, file.filename, file.content_type)
        logger.info(f"üìÑ Parsed file: {file.filename}, extracted {len(text)} chars")
        return {"success": True, "text": text, "filename": file.filename}
    except ValueError as e:
        logger.error(f"‚ùå File parsing failed: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        # SECURITY: Don't expose internal error details
        logger.error(f"‚ùå File parsing error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Failed to parse file. Please ensure the file is a valid PDF, DOCX, or TXT file."
        )


@app.post("/upload")
@limiter.limit("5/hour")  # SECURITY: Rate limiting (more restrictive for file uploads)
async def upload_and_process(
    request: Request,
    resume: UploadFile = File(...),
    job_description: UploadFile = File(None),
    jd_text: str = Form(None),
    _: bool = Depends(verify_api_key)  # SECURITY: Require API key
):
    """
    Upload resume file and optionally JD file or text
    
    Accepts:
    - Resume: PDF, DOCX, or TXT
    - Job Description: File or text
    """
    try:
        # Extract resume text
        resume_content = await resume.read()
        # SECURITY: Validate file before processing
        from utils.file_handlers import validate_file
        validate_file(resume_content, resume.filename, resume.content_type)
        resume_text = extract_text_from_file(resume_content, resume.filename, resume.content_type)
        
        # Get job description text
        if job_description:
            jd_content = await job_description.read()
            # SECURITY: Validate file before processing
            validate_file(jd_content, job_description.filename, job_description.content_type)
            jd_text_final = extract_text_from_file(jd_content, job_description.filename, job_description.content_type)
        elif jd_text:
            jd_text_final = jd_text
        else:
            raise HTTPException(
                status_code=400,
                detail="Either job_description file or jd_text must be provided"
            )
        
        # Process
        result = run_pipeline(resume_text, jd_text_final)
        
        return ProcessResponse(
            success=True,
            result=result
        )
    
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        # SECURITY: Don't expose internal error details
        logger.error(f"‚ùå Upload processing error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="An error occurred while processing your files. Please try again."
        )


@app.post("/download/resume/pdf")
@limiter.limit("20/hour")  # SECURITY: Rate limiting
async def download_optimized_resume_pdf(
    request: Request,
    text_request: TextProcessRequest,
    _: bool = Depends(verify_api_key)
):
    """Generate and download optimized resume as PDF"""
    
    try:
        result = run_pipeline(text_request.resume_text, text_request.job_description)
        
        # Create a modified resume with enhanced experience and reordered skills
        enhanced_resume = result.parsed_resume.model_copy(deep=True)
        enhanced_resume.experience = result.rewritten_resume.enhanced_experience
        enhanced_resume.skills = result.rewritten_resume.reordered_skills
        
        pdf_bytes = generate_resume_pdf(
            enhanced_resume,  # ‚úÖ Use enhanced version
            result.rewritten_resume
        )
        
        return StreamingResponse(
            io.BytesIO(pdf_bytes),
            media_type="application/pdf",
            headers={
                "Content-Disposition": f"attachment; filename=optimized_resume.pdf"
            }
        )
    
    except Exception as e:
        logger.error(f"‚ùå PDF generation error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="An error occurred while generating the PDF. Please try again."
        )


@app.post("/download/resume/word")
@limiter.limit("20/hour")  # SECURITY: Rate limiting
async def download_optimized_resume_word(
    request: Request,
    text_request: TextProcessRequest,
    _: bool = Depends(verify_api_key)
):
    """Generate and download optimized resume as Word document"""
    
    try:
        result = run_pipeline(text_request.resume_text, text_request.job_description)
        
        # Create a modified resume with enhanced experience and reordered skills
        enhanced_resume = result.parsed_resume.model_copy(deep=True)
        enhanced_resume.experience = result.rewritten_resume.enhanced_experience
        enhanced_resume.skills = result.rewritten_resume.reordered_skills
        
        docx_bytes = generate_resume_word(
            enhanced_resume,  # ‚úÖ Use enhanced version
            result.rewritten_resume
        )
        
        return StreamingResponse(
            io.BytesIO(docx_bytes),
            media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            headers={
                "Content-Disposition": f"attachment; filename=optimized_resume.docx"
            }
        )
    
    except Exception as e:
        logger.error(f"‚ùå Word document generation error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="An error occurred while generating the document. Please try again."
        )


# Legacy endpoint for backward compatibility
@app.post("/download/resume")
@limiter.limit("20/hour")  # SECURITY: Rate limiting
async def download_optimized_resume(
    request: Request,
    text_request: TextProcessRequest,
    _: bool = Depends(verify_api_key)
):
    """Generate and download optimized resume as PDF (legacy endpoint)"""
    # Reuse the PDF endpoint logic
    try:
        result = run_pipeline(text_request.resume_text, text_request.job_description)
        
        # Create a modified resume with enhanced experience and reordered skills
        enhanced_resume = result.parsed_resume.model_copy(deep=True)
        enhanced_resume.experience = result.rewritten_resume.enhanced_experience
        enhanced_resume.skills = result.rewritten_resume.reordered_skills
        
        pdf_bytes = generate_resume_pdf(
            enhanced_resume,
            result.rewritten_resume
        )
        
        return StreamingResponse(
            io.BytesIO(pdf_bytes),
            media_type="application/pdf",
            headers={
                "Content-Disposition": f"attachment; filename=optimized_resume.pdf"
            }
        )
    
    except Exception as e:
        logger.error(f"‚ùå PDF generation error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="An error occurred while generating the PDF. Please try again."
        )


@app.post("/download/cover-letter")
@limiter.limit("20/hour")  # SECURITY: Rate limiting
async def download_cover_letter(
    request: Request,
    text_request: TextProcessRequest,
    _: bool = Depends(verify_api_key)
):
    """Generate and download cover letter as PDF"""
    
    try:
        result = run_pipeline(text_request.resume_text, text_request.job_description)
        
        pdf_bytes = generate_cover_letter_pdf(
            result.cover_letter.content,
            result.parsed_resume.name or "Candidate"
        )
        
        return StreamingResponse(
            io.BytesIO(pdf_bytes),
            media_type="application/pdf",
            headers={
                "Content-Disposition": f"attachment; filename=cover_letter.pdf"
            }
        )
    
    except Exception as e:
        logger.error(f"‚ùå Cover letter generation error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="An error occurred while generating the cover letter. Please try again."
        )


@app.post("/analyze/resume-only")
@limiter.limit("20/hour")  # SECURITY: Rate limiting
async def analyze_resume_only(
    request: Request,
    resume_text: str = Form(..., min_length=10, max_length=500000),
    _: bool = Depends(verify_api_key)
):
    """Analyze resume without job description"""
    
    try:
        resume_parser = ResumeParserAgent()
        skill_agent = SkillAgent()
        
        parsed_resume = resume_parser.parse(resume_text)
        enhanced_skills = skill_agent.enhance_skills(parsed_resume)
        
        return {
            "parsed_resume": parsed_resume.model_dump(),
            "enhanced_skills": enhanced_skills
        }
    
    except Exception as e:
        logger.error(f"‚ùå Resume analysis error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="An error occurred while analyzing the resume. Please try again."
        )


@app.post("/analyze/jd-only")
@limiter.limit("20/hour")  # SECURITY: Rate limiting
async def analyze_jd_only(
    request: Request,
    job_description: str = Form(..., min_length=10, max_length=500000),
    _: bool = Depends(verify_api_key)
):
    """Analyze job description only"""
    
    try:
        jd_analyzer = JDAnalyzerAgent()
        parsed_jd = jd_analyzer.analyze(job_description)
        
        return parsed_jd.model_dump()
    
    except Exception as e:
        logger.error(f"‚ùå JD analysis error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="An error occurred while analyzing the job description. Please try again."
        )


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "main:app",
        host=os.getenv("HOST", "0.0.0.0"),
        port=int(os.getenv("PORT", 8000)),
        reload=os.getenv("DEBUG", "true").lower() == "true"
    )

