"""
AutoApply AI - FastAPI Backend
Multi-agent system for resume optimization and ATS scoring
"""
import os
import time
import uuid
import logging
from pathlib import Path
from typing import Optional
from contextlib import asynccontextmanager
from dotenv import load_dotenv

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("AutoApplyAI")

# Load environment variables from root .env file
env_path = Path(__file__).resolve().parent.parent / ".env"
load_dotenv(env_path)

from fastapi import FastAPI, File, UploadFile, Form, HTTPException, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
import io

# Import agents (partially generated by Cline CLI)
from agents import (
    ResumeParserAgent,
    JDAnalyzerAgent,
    GapAnalysisAgent,
    SkillAgent,
    OumiATSClassifier,  # Fine-tuned ATS model
    ATSScorerAgent,
    ResumeRewriteAgent,
    CoverLetterAgent,
    ExplanationAgent,
    ProjectRecommendationAgent
)

# Import models
from models.schemas import (
    ProcessRequest, ProcessResponse, AutoApplyResult,
    ParsedResume, ParsedJobDescription
)

# Import utilities
from utils.file_handlers import extract_text_from_file
from utils.pdf_generator import generate_resume_pdf, generate_cover_letter_pdf, generate_resume_word


# Store for async processing results
processing_results = {}


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler"""
    # Startup
    logger.info("üöÄ AutoApply AI Backend Starting...")
    logger.info(f"üìã Multi-agent pipeline ready")
    logger.info(f"üîë Together API Key: {'‚úì Set' if os.getenv('TOGETHER_API_KEY') else '‚úó Missing'}")
    yield
    # Shutdown
    logger.info("üëã AutoApply AI Backend Shutting Down...")


app = FastAPI(
    title="AutoApply AI",
    description="AI-powered resume optimization and ATS scoring system",
    version="1.0.0",
    lifespan=lifespan
)

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Request logging middleware
@app.middleware("http")
async def log_requests(request: Request, call_next):
    """Log all incoming requests"""
    start_time = time.time()
    
    # Log request
    logger.info(f"üì• {request.method} {request.url.path}")
    
    try:
        response = await call_next(request)
        
        # Log response
        process_time = time.time() - start_time
        logger.info(f"üì§ {request.method} {request.url.path} - {response.status_code} ({process_time:.2f}s)")
        
        return response
    except Exception as e:
        logger.error(f"‚ùå {request.method} {request.url.path} - Error: {str(e)}")
        raise


class TextProcessRequest(BaseModel):
    """Request model for text-based processing"""
    resume_text: str
    job_description: str


def run_pipeline(resume_text: str, jd_text: str) -> AutoApplyResult:
    """
    Execute the full multi-agent pipeline
    
    Pipeline Flow (orchestrated by Kestra):
    1. Parse Resume
    2. Analyze Job Description
    3. Gap Analysis
    4. Skill Enhancement
    5. OUMI ATS Classification (fine-tuned model)
    6. Together AI ATS Scoring
    7. Resume Rewrite
    8. Cover Letter Generation
    9. Explanation
    10. Project Recommendations
    """
    
    start_time = time.time()
    logger.info(f"üöÄ Starting pipeline - Resume: {len(resume_text)} chars, JD: {len(jd_text)} chars")
    
    # Initialize agents (partially generated by Cline CLI)
    resume_parser = ResumeParserAgent()
    jd_analyzer = JDAnalyzerAgent()
    gap_analyzer = GapAnalysisAgent()
    skill_agent = SkillAgent()
    oumi_classifier = OumiATSClassifier()  # Fine-tuned ATS model
    ats_scorer = ATSScorerAgent()
    resume_rewriter = ResumeRewriteAgent()
    cover_letter_agent = CoverLetterAgent()
    explanation_agent = ExplanationAgent()
    project_agent = ProjectRecommendationAgent()
    
    # Step 1: Parse Resume
    logger.info("üìÑ Step 1: Parsing resume...")
    parsed_resume = resume_parser.parse(resume_text)
    logger.info(f"   ‚úì Found {len(parsed_resume.skills)} skills, {len(parsed_resume.experience)} experiences")
    
    # Step 2: Analyze Job Description
    logger.info("üìã Step 2: Analyzing job description...")
    parsed_jd = jd_analyzer.analyze(jd_text)
    logger.info(f"   ‚úì Role: {parsed_jd.role}, Required skills: {len(parsed_jd.required_skills)}")
    
    # Step 3: Gap Analysis
    logger.info("üîç Step 3: Performing gap analysis...")
    gap_analysis = gap_analyzer.analyze(parsed_resume, parsed_jd)
    logger.info(f"   ‚úì Match: {gap_analysis.overall_match_percentage}%")
    
    # Step 4: Skill Enhancement (ethical)
    logger.info("‚ú® Step 4: Enhancing skills...")
    enhanced_skills = skill_agent.enhance_skills(parsed_resume)
    
    # Update resume with standardized skills if available
    if enhanced_skills.get("standardized_skills"):
        parsed_resume.skills = enhanced_skills["standardized_skills"]
    logger.info(f"   ‚úì Skills enhanced")
    
    # Step 5: OUMI ATS Classification (provides signals for Together AI)
    logger.info("üéØ Step 5: OUMI ATS classification...")
    oumi_result = oumi_classifier.classify(parsed_resume, parsed_jd)
    logger.info(f"   ‚úì ATS Bucket: {oumi_result.get('ats_bucket', 'N/A')}")
    
    # Step 6: Together AI ATS Scoring (enhanced with OUMI signals)
    logger.info("üìä Step 6: ATS scoring...")
    ats_score = ats_scorer.score(parsed_resume, parsed_jd, gap_analysis)
    logger.info(f"   ‚úì ATS Score: {ats_score.overall_score}/100 ({ats_score.bucket.value})")
    logger.info(f"   üìä Components: skill={ats_score.skill_match_score}, keyword={ats_score.keyword_score}, format={ats_score.formatting_score}, exp={ats_score.experience_alignment_score}")
    
    # Debug: Calculate expected score
    expected = (
        ats_score.skill_match_score * 0.40 +
        ats_score.keyword_score * 0.20 +
        ats_score.formatting_score * 0.20 +
        ats_score.experience_alignment_score * 0.20
    )
    logger.info(f"   üìê Expected (before penalties): {expected:.1f}")
    
    if ats_score.issues:
        critical = len([i for i in ats_score.issues if i.severity == "critical"])
        high = len([i for i in ats_score.issues if i.severity == "high"])
        logger.info(f"   ‚ö†Ô∏è Issues: {critical} critical, {high} high")
    
    # Step 7: Resume Rewrite
    logger.info("‚úçÔ∏è Step 7: Rewriting resume...")
    rewritten_resume = resume_rewriter.rewrite(
        parsed_resume, parsed_jd, gap_analysis, ats_score
    )
    logger.info("   ‚úì Resume rewritten")
    
    # Step 8: Cover Letter Generation
    logger.info("üìù Step 8: Generating cover letter...")
    cover_letter = cover_letter_agent.generate(
        parsed_resume, parsed_jd, gap_analysis
    )
    logger.info(f"   ‚úì Cover letter: {cover_letter.word_count} words")
    
    # Step 9: Explanation
    logger.info("üí° Step 9: Generating explanation...")
    explanation = explanation_agent.explain(
        parsed_resume, parsed_jd, gap_analysis, ats_score
    )
    logger.info("   ‚úì Explanation generated")
    
    # Step 10: Project Recommendations
    logger.info("üìö Step 10: Generating project recommendations...")
    project_recommendations = project_agent.recommend(gap_analysis)
    logger.info(f"   ‚úì {len(project_recommendations.recommended_projects)} projects recommended")
    
    processing_time = time.time() - start_time
    logger.info(f"‚úÖ Pipeline complete! Total time: {processing_time:.2f}s")
    
    return AutoApplyResult(
        parsed_resume=parsed_resume,
        parsed_jd=parsed_jd,
        gap_analysis=gap_analysis,
        ats_score=ats_score,
        rewritten_resume=rewritten_resume,
        cover_letter=cover_letter,
        explanation=explanation,
        project_recommendations=project_recommendations,
        processing_time=round(processing_time, 2)
    )


def process_async(workflow_id: str, resume_text: str, jd_text: str):
    """Background task for async processing"""
    logger.info(f"üîÑ Async processing started: {workflow_id}")
    try:
        result = run_pipeline(resume_text, jd_text)
        processing_results[workflow_id] = {
            "status": "completed",
            "result": result
        }
        logger.info(f"‚úÖ Async processing completed: {workflow_id}")
    except Exception as e:
        logger.error(f"‚ùå Async processing failed: {workflow_id} - {str(e)}")
        processing_results[workflow_id] = {
            "status": "failed",
            "error": str(e)
        }


@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "service": "AutoApply AI",
        "version": "1.0.0",
        "agents": [
            "ResumeParser",
            "JDAnalyzer",
            "GapAnalysis",
            "SkillAgent",
            "ATSScorer",
            "ResumeRewriter",
            "CoverLetter",
            "Explanation",
            "ProjectRecommendation"
        ]
    }


@app.post("/process", response_model=ProcessResponse)
async def process_resume(request: TextProcessRequest):
    """
    Process resume and job description (synchronous)
    
    This endpoint runs the full multi-agent pipeline and returns results.
    """
    logger.info(f"üì• Process request received - Resume: {len(request.resume_text)} chars, JD: {len(request.job_description)} chars")
    
    try:
        result = run_pipeline(request.resume_text, request.job_description)
        logger.info("‚úÖ Process request completed successfully")
        
        return ProcessResponse(
            success=True,
            result=result
        )
    
    except Exception as e:
        logger.error(f"‚ùå Process request failed: {str(e)}", exc_info=True)
        return ProcessResponse(
            success=False,
            error=str(e)
        )


@app.post("/process/async")
async def process_resume_async(
    request: TextProcessRequest,
    background_tasks: BackgroundTasks
):
    """
    Process resume and job description (asynchronous)
    
    Returns a workflow ID that can be used to check status.
    """
    workflow_id = str(uuid.uuid4())
    
    processing_results[workflow_id] = {"status": "processing"}
    
    background_tasks.add_task(
        process_async,
        workflow_id,
        request.resume_text,
        request.job_description
    )
    
    return {
        "workflow_id": workflow_id,
        "status": "processing",
        "message": "Resume processing started. Use /status/{workflow_id} to check progress."
    }


@app.get("/status/{workflow_id}")
async def check_status(workflow_id: str):
    """Check the status of an async processing job"""
    
    if workflow_id not in processing_results:
        raise HTTPException(status_code=404, detail="Workflow not found")
    
    result = processing_results[workflow_id]
    
    if result["status"] == "completed":
        return ProcessResponse(
            success=True,
            workflow_id=workflow_id,
            result=result["result"]
        )
    elif result["status"] == "failed":
        return ProcessResponse(
            success=False,
            workflow_id=workflow_id,
            error=result.get("error", "Unknown error")
        )
    else:
        return {"workflow_id": workflow_id, "status": "processing"}


@app.post("/parse-file")
async def parse_file(file: UploadFile = File(...)):
    """
    Parse a file (PDF, DOCX, TXT) and return extracted text.
    Used by frontend to properly extract resume text.
    """
    try:
        content = await file.read()
        text = extract_text_from_file(content, file.filename)
        logger.info(f"üìÑ Parsed file: {file.filename}, extracted {len(text)} chars")
        return {"success": True, "text": text, "filename": file.filename}
    except ValueError as e:
        logger.error(f"‚ùå File parsing failed: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"‚ùå File parsing error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to parse file: {str(e)}")


@app.post("/upload")
async def upload_and_process(
    resume: UploadFile = File(...),
    job_description: UploadFile = File(None),
    jd_text: str = Form(None)
):
    """
    Upload resume file and optionally JD file or text
    
    Accepts:
    - Resume: PDF, DOCX, or TXT
    - Job Description: File or text
    """
    try:
        # Extract resume text
        resume_content = await resume.read()
        resume_text = extract_text_from_file(resume_content, resume.filename)
        
        # Get job description text
        if job_description:
            jd_content = await job_description.read()
            jd_text_final = extract_text_from_file(jd_content, job_description.filename)
        elif jd_text:
            jd_text_final = jd_text
        else:
            raise HTTPException(
                status_code=400,
                detail="Either job_description file or jd_text must be provided"
            )
        
        # Process
        result = run_pipeline(resume_text, jd_text_final)
        
        return ProcessResponse(
            success=True,
            result=result
        )
    
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/download/resume/pdf")
async def download_optimized_resume_pdf(request: TextProcessRequest):
    """Generate and download optimized resume as PDF"""
    
    try:
        result = run_pipeline(request.resume_text, request.job_description)
        
        # Create a modified resume with enhanced experience and reordered skills
        enhanced_resume = result.parsed_resume.model_copy(deep=True)
        enhanced_resume.experience = result.rewritten_resume.enhanced_experience
        enhanced_resume.skills = result.rewritten_resume.reordered_skills
        
        pdf_bytes = generate_resume_pdf(
            enhanced_resume,  # ‚úÖ Use enhanced version
            result.rewritten_resume
        )
        
        return StreamingResponse(
            io.BytesIO(pdf_bytes),
            media_type="application/pdf",
            headers={
                "Content-Disposition": f"attachment; filename=optimized_resume.pdf"
            }
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/download/resume/word")
async def download_optimized_resume_word(request: TextProcessRequest):
    """Generate and download optimized resume as Word document"""
    
    try:
        result = run_pipeline(request.resume_text, request.job_description)
        
        # Create a modified resume with enhanced experience and reordered skills
        enhanced_resume = result.parsed_resume.model_copy(deep=True)
        enhanced_resume.experience = result.rewritten_resume.enhanced_experience
        enhanced_resume.skills = result.rewritten_resume.reordered_skills
        
        docx_bytes = generate_resume_word(
            enhanced_resume,  # ‚úÖ Use enhanced version
            result.rewritten_resume
        )
        
        return StreamingResponse(
            io.BytesIO(docx_bytes),
            media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            headers={
                "Content-Disposition": f"attachment; filename=optimized_resume.docx"
            }
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# Legacy endpoint for backward compatibility
@app.post("/download/resume")
async def download_optimized_resume(request: TextProcessRequest):
    """Generate and download optimized resume as PDF (legacy endpoint)"""
    return await download_optimized_resume_pdf(request)


@app.post("/download/cover-letter")
async def download_cover_letter(request: TextProcessRequest):
    """Generate and download cover letter as PDF"""
    
    try:
        result = run_pipeline(request.resume_text, request.job_description)
        
        pdf_bytes = generate_cover_letter_pdf(
            result.cover_letter.content,
            result.parsed_resume.name or "Candidate"
        )
        
        return StreamingResponse(
            io.BytesIO(pdf_bytes),
            media_type="application/pdf",
            headers={
                "Content-Disposition": f"attachment; filename=cover_letter.pdf"
            }
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/analyze/resume-only")
async def analyze_resume_only(resume_text: str = Form(...)):
    """Analyze resume without job description"""
    
    try:
        resume_parser = ResumeParserAgent()
        skill_agent = SkillAgent()
        
        parsed_resume = resume_parser.parse(resume_text)
        enhanced_skills = skill_agent.enhance_skills(parsed_resume)
        
        return {
            "parsed_resume": parsed_resume.model_dump(),
            "enhanced_skills": enhanced_skills
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/analyze/jd-only")
async def analyze_jd_only(job_description: str = Form(...)):
    """Analyze job description only"""
    
    try:
        jd_analyzer = JDAnalyzerAgent()
        parsed_jd = jd_analyzer.analyze(job_description)
        
        return parsed_jd.model_dump()
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "main:app",
        host=os.getenv("HOST", "0.0.0.0"),
        port=int(os.getenv("PORT", 8000)),
        reload=os.getenv("DEBUG", "true").lower() == "true"
    )

